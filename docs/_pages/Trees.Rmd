---
title: "More modelling recipes"
subtitle: "Decision trees"
author: "[Pat Reen](https://www.linkedin.com/in/patrick-reen/)"
---

```{r xgboost for incidence prediction}
library(mlr3)
library(xgboost)

samp_cv <- sample(1:nrow(df_test),100000)
df_cv <- df_test[samp_cv,]
df_test2 <- df_test[-samp_cv,]

# One hot encoding
train_label <- df_train$inc_count_tot
cv_label <- df_cv$inc_count_tot
test_label <- df_test2$inc_count_tot

new_train <- model.matrix(inc_count_tot  ~ 
                        cal_year
                      + policy_year
                      + sex                           
                      + smoker
                      + benefit_period
                      + waiting_period
                      + occupation
                      + age          
                      + sum_assured             
                       , data = df_train)

new_cv <- model.matrix(inc_count_tot  ~ 
                        cal_year
                      + policy_year
                      + sex                           
                      + smoker
                      + benefit_period
                      + waiting_period
                      + occupation
                      + age          
                      + sum_assured             
                       , data = df_cv)

new_test <- model.matrix(inc_count_tot  ~ 
                        cal_year
                      + policy_year
                      + sex                           
                      + smoker
                      + benefit_period
                      + waiting_period
                      + occupation
                      + age          
                      + sum_assured             
                       , data = df_test2)


# prepare matrix 
DM_train <- xgb.DMatrix(data = new_train, label = train_label) 
DM_cv <- xgb.DMatrix(data = new_cv, label = cv_label)
DM_test <- xgb.DMatrix(data = new_test, label = test_label)

# default hyper-parameters. Given the dataset is very imbalanced, we use 'scale_pos_weight' to re-balance it which is the number of negative observations (i.e. "0") over the number of positive observations (i.e. "1").
params <- list(booster = "gbtree"
               , objective = "binary:logistic"
               , eta=0.1
               , gamma=0
               , max_depth=6
               , min_child_weight=1
               , subsample=1
               , colsample_bytree=1
               , scale_pos_weight=4475/24)


# xgb training with watchlist to show cross-validation
xgb1 <- xgb.train(params = params, data = DM_train, nrounds = 10000, watchlist = list(train=DM_train, val=DM_cv)
                   ,print_every_n = 10
                   ,early_stopping_rounds = 100
                   ,maximize = F, eval_metric = "error")

summary(xgb1)
xgb_pred1 <- predict (xgb1, DM_test)

require(caret)
confusionMatrix(as.factor(as.numeric(xgb_pred1 >0.5)), as.factor(test_label))

require(pROC)
test_roc = roc(test_label ~ xgb_pred1, plot = TRUE, print.auc = TRUE)


# xgb.cv module automatically divides the data into 'nfold' and performs cross-validation, thus to reduce over-fitting
xgbcv <- xgb.cv(params = params, data = DM_train
                 , nrounds = 10000
                 , nfold = 5
                 , showsd = T
                 , stratified = T
                 , print_every_n = 10
                 , early_stopping_rounds = 50
                 , maximize = F, eval_metric = "error")

summary(xgbcv)
```