[
  {
    "path": "posts/20211118 trees/",
    "title": "Decision trees",
    "description": "Decision tree applications.",
    "author": [
      {
        "name": "Pat Reen",
        "url": "https://www.linkedin.com/in/patrick-reen/"
      }
    ],
    "date": "2021-11-18",
    "categories": [],
    "contents": "\r\n\r\nContents\r\nOverview\r\nBackground\r\nIntroduction\r\n\r\n\r\nOverview\r\nBackground\r\nLorem ipsum dolor sit amet, consectetur adipisicing elit. Iste quas, esse laudantium quis ipsa consequuntur iure atque! Necessitatibus quam quidem illum, corrupti molestiae, maxime neque, consequuntur quia alias, modi commodi.Lorem ipsum dolor sit amet, consectetur adipisicing elit. Cupiditate rem ducimus dolores repellat itaque perferendis corporis ex dicta doloremque optio harum excepturi, ut, praesentium asperiores! Voluptatum amet perspiciatis iusto, fugiat!Lorem ipsum dolor sit amet, consectetur adipisicing elit. Harum, libero quia placeat necessitatibus culpa mollitia, illum, quam sed ratione ad eaque suscipit consectetur itaque quasi aperiam. Quaerat, nihil voluptas tenetur?Lorem ipsum dolor sit amet, consectetur adipisicing elit. Eos unde quam autem fugit, aliquam perspiciatis, accusamus numquam, pariatur impedit excepturi debitis repellendus consectetur laboriosam voluptatem temporibus sunt illo magnam! Voluptates.Lorem ipsum dolor sit amet, consectetur adipisicing elit. Ullam repudiandae quidem, fuga eaque, deserunt eius. Aperiam, maxime fuga alias ad vel amet et facere soluta asperiores quasi. A, aperiam, ipsam.\r\nIntroduction\r\nLorem ipsum dolor sit amet, consectetur adipisicing elit. Iste quas, esse laudantium quis ipsa consequuntur iure atque! Necessitatibus quam quidem illum, corrupti molestiae, maxime neque, consequuntur quia alias, modi commodi.Lorem ipsum dolor sit amet, consectetur adipisicing elit. Cupiditate rem ducimus dolores repellat itaque perferendis corporis ex dicta doloremque optio harum excepturi, ut, praesentium asperiores! Voluptatum amet perspiciatis iusto, fugiat!Lorem ipsum dolor sit amet, consectetur adipisicing elit. Harum, libero quia placeat necessitatibus culpa mollitia, illum, quam sed ratione ad eaque suscipit consectetur itaque quasi aperiam. Quaerat, nihil voluptas tenetur?Lorem ipsum dolor sit amet, consectetur adipisicing elit. Eos unde quam autem fugit, aliquam perspiciatis, accusamus numquam, pariatur impedit excepturi debitis repellendus consectetur laboriosam voluptatem temporibus sunt illo magnam! Voluptates.Lorem ipsum dolor sit amet, consectetur adipisicing elit. Ullam repudiandae quidem, fuga eaque, deserunt eius. Aperiam, maxime fuga alias ad vel amet et facere soluta asperiores quasi. A, aperiam, ipsam.\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2021-11-19T16:29:57+11:00",
    "input_file": "trees.knit.md"
  },
  {
    "path": "posts/20211117 bayesian_regression/",
    "title": "Bayesian regression",
    "description": "Bayesian regression applications.",
    "author": [
      {
        "name": "Pat Reen",
        "url": "https://www.linkedin.com/in/patrick-reen/"
      }
    ],
    "date": "2021-11-17",
    "categories": [],
    "contents": "\r\n\r\nContents\r\nOverview\r\nBackground\r\nIntroduction\r\n\r\n\r\nOverview\r\nBackground\r\nLorem ipsum dolor sit amet, consectetur adipisicing elit. Iste quas, esse laudantium quis ipsa consequuntur iure atque! Necessitatibus quam quidem illum, corrupti molestiae, maxime neque, consequuntur quia alias, modi commodi.Lorem ipsum dolor sit amet, consectetur adipisicing elit. Cupiditate rem ducimus dolores repellat itaque perferendis corporis ex dicta doloremque optio harum excepturi, ut, praesentium asperiores! Voluptatum amet perspiciatis iusto, fugiat!Lorem ipsum dolor sit amet, consectetur adipisicing elit. Harum, libero quia placeat necessitatibus culpa mollitia, illum, quam sed ratione ad eaque suscipit consectetur itaque quasi aperiam. Quaerat, nihil voluptas tenetur?Lorem ipsum dolor sit amet, consectetur adipisicing elit. Eos unde quam autem fugit, aliquam perspiciatis, accusamus numquam, pariatur impedit excepturi debitis repellendus consectetur laboriosam voluptatem temporibus sunt illo magnam! Voluptates.Lorem ipsum dolor sit amet, consectetur adipisicing elit. Ullam repudiandae quidem, fuga eaque, deserunt eius. Aperiam, maxime fuga alias ad vel amet et facere soluta asperiores quasi. A, aperiam, ipsam.\r\nIntroduction\r\nLorem ipsum dolor sit amet, consectetur adipisicing elit. Iste quas, esse laudantium quis ipsa consequuntur iure atque! Necessitatibus quam quidem illum, corrupti molestiae, maxime neque, consequuntur quia alias, modi commodi.Lorem ipsum dolor sit amet, consectetur adipisicing elit. Cupiditate rem ducimus dolores repellat itaque perferendis corporis ex dicta doloremque optio harum excepturi, ut, praesentium asperiores! Voluptatum amet perspiciatis iusto, fugiat!Lorem ipsum dolor sit amet, consectetur adipisicing elit. Harum, libero quia placeat necessitatibus culpa mollitia, illum, quam sed ratione ad eaque suscipit consectetur itaque quasi aperiam. Quaerat, nihil voluptas tenetur?Lorem ipsum dolor sit amet, consectetur adipisicing elit. Eos unde quam autem fugit, aliquam perspiciatis, accusamus numquam, pariatur impedit excepturi debitis repellendus consectetur laboriosam voluptatem temporibus sunt illo magnam! Voluptates.Lorem ipsum dolor sit amet, consectetur adipisicing elit. Ullam repudiandae quidem, fuga eaque, deserunt eius. Aperiam, maxime fuga alias ad vel amet et facere soluta asperiores quasi. A, aperiam, ipsam.\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2021-11-19T16:30:29+11:00",
    "input_file": "bayesian_regression.knit.md"
  },
  {
    "path": "posts/20211108 life_stats/",
    "title": "Life industry stats",
    "description": "This article sets out a simple recipe for visualising Australian life industry data from APRA using Tableau.",
    "author": [
      {
        "name": "Pat Reen",
        "url": "https://www.linkedin.com/in/patrick-reen/"
      }
    ],
    "date": "2021-11-08",
    "categories": [],
    "contents": "\r\n\r\nContents\r\nBackground\r\nIntroducing radial bar charts\r\nUsing r to view industry stats\r\n\r\nBackground\r\nThis article sets out a simple recipe for visualising Australian life industry data from APRA using Tableau.\r\nTableau public is a free to use visualisation tool that can ingest data in a number of different formats and is useful at creating flexible visualisation. Gephi is an open source graphing platform that has a number of algorithms that can automatically spread the points (nodes) of a network and their connecting lines (edges) into set of points that can be combined with other data and visualised in Tableau. You could also create a visual in Gephi, but Tableau has a number of other useful tools.\r\nSee link above to GitHub repository which has data and tableau workbook for this recipe.\r\nIntroducing radial bar charts\r\nA radial bar chart is a bar chart plotted in polar co-ordinates rather than a Cartesian plane. This site sets out a very simple approach, which I have used here.\r\nUsing r to view industry stats\r\nAs with most data sources, an alternative visualisation tool is r. The below extracts the individual disability income\r\n\r\n\r\nShow code\r\n\r\n# this script reads in the APRA data and produces a few graphs for presentation\r\n\r\npackages <- c(\"tidyverse\", \"ggplot2\", \"readxl\", \"lubridate\", \"scales\", \"xtable\")\r\ninstall.packages(setdiff(packages, rownames(installed.packages())))  \r\nfor (package in packages) {\r\n    library(package, character.only = TRUE)\r\n}\r\n\r\npercent0 <- function(value) {\r\n  return(percent(value, accuracy = 0.1))\r\n}\r\n\r\n# update names\r\n# [1] \"Reporting date\"      \"Industry sector\"     \"Subject\"\r\n# [4] \"Category\"            \"Data item\"           \"Reporting Structure\"\r\n# [7] \"Class of business\"   \"Product Group\"       \"Calculation basis\"\r\n# [10] \"Value\"               \"Notes\"\r\n\r\nqrt_col_name <- c(\r\n  \"rep_date\", \"sector\", \"subject\", \"category\", \"data_item\",\r\n  \"rep_struc\", \"class\", \"product\", \"calc_basis\", \"value\",\r\n  \"notes\"\r\n)\r\nqrt_col_type <- c(\"date\", rep(\"text\", 8), \"numeric\", \"text\")\r\n\r\n#---------- read in quarterly data\r\nqrt_data <- read_xlsx(\r\n  path = \"Quarterly life insurance performance statistics database - June 2008 to June 2021.xlsx\",\r\n  sheet = \"Data\", \r\n  col_names = qrt_col_name, \r\n  skip = 1, \r\n  col_types = qrt_col_type,\r\n  trim_ws = TRUE, na = \"N/A\"\r\n)\r\n\r\n# add fiscal years\r\nqrt_data$fin_year <- paste0(\"FY\", format(year(qrt_data$rep_date) +\r\n  as.integer(month(qrt_data$rep_date) > 6)))\r\n# also add calendar year as an alternative aggregation\r\nqrt_data$cal_year <- format(year(qrt_data$rep_date))\r\n\r\n\r\n\r\n# DI Profit by Year -----------------------------------------------------\r\n\r\nDI_risk_type <- \"Individual Disability Income Insurance\"\r\n\r\ndata_items <- c(\r\n    \"Profit / loss before tax ($m)\" = \"Profit / loss before tax\",\r\n    \"Premiums after reinsurance ($m)\" = \"Net policy revenue\",\r\n    \"Premiums before reinsurance ($m)\" = \"Gross policy revenue\"\r\n)\r\n\r\nDI_profit <- qrt_data %>%\r\n  filter(data_item %in% data_items) %>%\r\n  filter(is.na(class)) %>%\r\n  filter(product == DI_risk_type) %>%\r\n  group_by(`Fin year` = fin_year, data_item) %>%\r\n  summarise(risk_value = sum(value)) %>%\r\n  spread(data_item, risk_value) %>% \r\n  mutate(`Margin (%)` = percent0(`Profit / loss before tax` /\r\n  `Net policy revenue`)) %>%\r\n  rename(data_items)\r\n\r\nDI_profit_print <- xtable(\r\n    x = DI_profit,\r\n    caption = \"Individual Disability Income Industry Profit\",\r\n    align = \"llrrrr\",\r\n    digits = 0\r\n)\r\n\r\nprint(DI_profit_print, \r\n      type = \"html\",\r\n      file = \"DI_profit\",\r\n      include.rownames = FALSE,\r\n      ) \r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/20211108 life_stats/preview.png",
    "last_modified": "2021-11-19T14:55:22+11:00",
    "input_file": "life_stats.knit.md",
    "preview_width": 697,
    "preview_height": 673
  },
  {
    "path": "posts/20211026 network_diagram/",
    "title": "Network graphs",
    "description": "This article sets out a simple recipe for visualising a network using Tableau and Gephi. The example used is the network of bands and band members interconnected with Nirvana.",
    "author": [
      {
        "name": "Pat Reen",
        "url": "https://www.linkedin.com/in/patrick-reen/"
      }
    ],
    "date": "2021-10-26",
    "categories": [],
    "contents": "\r\n\r\nContents\r\nBackground\r\nThe result\r\nWhat is a network chart?\r\nStructuring data for Gephi\r\nPlotting network points in Gephi\r\n\r\nExporting data\r\n\r\nBackground\r\nThis article sets out a simple recipe for visualising a network using Tableau and Gephi. The example used is the network of bands and band members interconnected with Nirvana.\r\nTableau public is a free to use visualisation tool that can ingest data in a number of different formats and is useful at creating flexible visualisation. Gephi is an open source graphing platform that has a number of algorithms that can automatically spread the points (nodes) of a network and their connecting lines (edges) into set of points that can be combined with other data and visualised in Tableau. You could also create a visual in Gephi, but Tableau has a number of other useful tools.\r\nThe inspiration for this approach to network charts came from this example.\r\nSee link above to GitHub repository which has data and tableau workbook for this recipe.\r\nThe result\r\nA preview of the final visualisation is below, with a link here\r\n\r\nWhat is a network chart?\r\nA visual representation of a network. It comprises nodes (points in the graph) and edges (lines joining the points).\r\nStructuring data for Gephi\r\nGephi needs two tables: (1) a table of nodes (here individual band members) and (2) a table of edges i.e. a definition of how those nodes relate to each other.\r\nOur data looks like the below - a record for each band member for a given iteration of the band. Each record has an ID and each record has a parent (the other point we would like that record to connect to). This data table fully defines the data as well as each row’s relationship with the other rows.\r\n\r\nFrom this data we create two additional tables. The “nodes” table contains each row’s ID and its Node (the same value as the ID):\r\n\r\nThe “edges” table contains each row’s ID its parent or Target ID:\r\n\r\nThis is all that Gephi needs (note: Gephi needs the tables as values).\r\nPlotting network points in Gephi\r\nInstall and open Gephi (see “inspiration” link above for troubleshooting tips on installation). The Gephi start page looks like:\r\n\r\nYou’ll want to open a file, loading the nodes (as a “nodes table”) first:\r\n Load the data into a new workspace as a “Directed” graph type.\r\n\r\nLoad the edges data into the same workspace as an “Edges” table.\r\n\r\nYou should get something that looks like this in the “Overview” page:\r\n\r\nThen, the magic of Gephi - the “layout” options will space out the points and edges considering the relationships you have specified. There are various layouts that suit different data and they can be tailored e.g. to space points further from each other or space dependent on the number of related points etc. In the end you might get something that looks like this:\r\n\r\nExporting data\r\nYou can then export the data. It is exported as a “.gexf” which can be opened in excel as a “.XML” file. Extract the X and Y points and join to your data - and there you have it - network points for Tableau.\r\nThe rest follows simply in the Tableau workbook provided in the GitHub page.\r\n\r\n\r\n\r\n",
    "preview": "posts/20211026 network_diagram/preview.png",
    "last_modified": "2021-11-19T16:32:46+11:00",
    "input_file": "network_diagram.knit.md",
    "preview_width": 691,
    "preview_height": 693
  },
  {
    "path": "posts/20211014 regression/",
    "title": "Data transformations and regression modelling",
    "description": "This article sets out a few practical recipes for modelling with (life) insurance data. Insurance events are typically of low probability - these recipes consider some of   the limitations of \"small data\" model fitting (where the observations of interest are sparse) and other topics for insurance like comparisons to standard tables.",
    "author": [
      {
        "name": "Pat Reen",
        "url": "https://www.linkedin.com/in/patrick-reen/"
      }
    ],
    "date": "2021-10-14",
    "categories": [],
    "contents": "\r\n\r\nContents\r\nOverview\r\nBackground\r\nLibraries\r\nFurther reading\r\n\r\nData Simulation\r\nSimulating policies\r\nExpected claim rate\r\nExpected claims\r\nGrouped data\r\n\r\nData Exploration\r\nData structure\r\nFactors\r\nSelection methods\r\nManipulation methods\r\nMissing data\r\nReview exposure data\r\nData format\r\nVisualisation methods\r\nIntroduction to ggplot\r\nLayers\r\nThemes\r\n3D visualisations with plotly\r\n\r\nReview claim data\r\n\r\nModel selection\r\nSplitting data\r\nTraining vs testing data\r\nClass imbalance\r\n\r\nRegression\r\nBackground\r\nLinear vs logistic regression\r\nLogistic regression for claims incidence\r\nLinear regression for claims incidence\r\n\r\n(Un)grouped data\r\nOffsets\r\n\r\nActuals or AvE?\r\nStepwise regression\r\nConfidence intervals\r\nPredictions\r\nOut of sample predictions\r\n\r\nBayesian regression\r\nOther classification models\r\n\r\nEvaluation\r\nTechniques\r\nAIC\r\nAnova\r\nLikelihood ratio test\r\nOther tests\r\n\r\nResidual checks\r\nStandard model plots\r\nAlternatives\r\nP-P plots\r\nOther tests\r\n\r\nConfusion matrix\r\nROC/ AUC\r\n\r\nReferences\r\n\r\nOverview\r\nBackground\r\nThis article sets out a few practical recipes for modelling with (life) insurance data. Insurance events are typically of low probability - these recipes consider some of the limitations of “small data” model fitting (where the observations of interest are sparse) and other topics for insurance like comparisons to standard tables. Themes\r\nCommon data transforms, summary stats, and simple visualisations\r\nRegression\r\nGrouped vs ungrouped data\r\nChoice of: response distribution, link (and offsets), explanatory variables\r\nModelling variance to industry/ reference (A/E or A - E)\r\nModel selection: stepwise regression, likelihood tests, model evaluation\r\nPredictions, confidence intervals and visualisations\r\n\r\nSee link above to GitHub repository which has the detailed code.\r\nLibraries\r\nA list of packages used in the recipe book.\r\n\r\n\r\nShow code\r\n\r\nlibrary(rmdformats) # theme for the HTML doc\r\nlibrary(bookdown)   # bibliography formatting\r\nlibrary(kableExtra) # formatting tables\r\nlibrary(scales)     # data formatting  \r\nlibrary(dplyr)      # tidyverse: data manipulation\r\nlibrary(tidyr)      # tidyverse: tidy messy data\r\nlibrary(corrplot)   # correlation matrix visualisation, optional\r\nlibrary(ggplot2)    # tidyverse: graphs\r\nlibrary(pdp)        # tidyverse: arranging plots\r\nlibrary(GGally)     # tidyverse: extension of ggplot2\r\nlibrary(ggthemes)   # tidyverse: additional themes for ggplot, optional\r\nlibrary(plotly)     # graphs, including 3D \r\nlibrary(caret)      # sampling techniques\r\nlibrary(broom)      # tidyverse: visualising statistical objects\r\nlibrary(pROC)       # visualising ROC curves\r\nlibrary(lmtest)     # tests for regression models\r\n\r\n# packages below have some interaction with earlier packages, not always needed\r\nlibrary(arm)        # binned residual plot\r\nlibrary(msme)       # statistical tests, pearson dispersion\r\nlibrary(MASS)       # statistics\r\n\r\n\r\n\r\nFurther reading\r\nA few books and articles of interest:\r\nR Markdown Cookbook has everything you need to know to set up an r markdown document like this one.\r\nGeneralised Linear Models for Insurance Data is a great book introducing GLMs in the context of insurance, considering problems specific to insurance data.\r\nTidyverse documentation full set of documentation for Tidyverse packages (packages for data science) e.g. dplyr for data manipulation; tidyr for tidying up messy data; ggplot for visualisation.\r\nData Simulation\r\nThis section sets out a method for generating dummy data. The simulated data is intended to reflect typical data used in an analysis of disability income incidence experience and is used throughout this analysis. Replace this data with your actual data.\r\nMore detail on the techniques used can be found in the section on data manipulation.\r\nSimulating policies\r\nWe start by simulating a mix of 200k policies over 3 years. Some simplifying assumptions e.g. nil lapse/ new bus (no allowance for part years of exposure), no indexation. Mix of business assumptions for benefit period, waiting period and occupation taken taken from (James Louw 2012), with the remainder based on an anecdotal view of industry mix not intended to be reflective of any one business.\r\n\r\n\r\nShow code\r\n\r\n# set the seed value (for the random number generator) so that the simulated data frame can be replicated later\r\nset.seed(10)\r\n# create 200k policies\r\nn <- 200000\r\n\r\n# data frame columns\r\n# policy_year skewed to early years, but tail is fat\r\ndf <- data.frame(id = c(1:n), cal_year = 2018,policy_year = round(rweibull(n, scale=5, shape=2),0)) \r\ndf <- df %>% mutate(sex = replicate(n,sample(c(\"m\",\"f\",\"u\"), size=1, replace=TRUE, prob=c(.75,.20,.05))),\r\nsmoker = replicate(n,sample(c(\"n\",\"s\",\"u\"), size=1, replace=TRUE, prob=c(.85,.1,.05))),\r\n# mix of business for benefit_period, waiting_period, occupation taken from industry presentation\r\nbenefit_period = replicate(n,sample(c(\"a65\",\"2yr\",\"5yr\"), size=1, replace=TRUE, prob=c(.76,.12,.12))),\r\nwaiting_period = replicate(n,sample(c(\"14d\",\"30d\",\"90d\",\"720d\"), size=1, replace=TRUE, prob=c(.04,.7,.15,.11))),\r\noccupation = replicate(n,sample(c(\"prof\",\"sed\",\"techn\",\"blue\",\"white\"), size=1, replace=TRUE, prob=c(.4,.2,.2,.1,.1))),\r\n# age and policy year correlated; age normally distributed around 40 + policy_year (where policy_year is distributed around 5 years), floored at 25, capped at 60\r\nage = round(pmax(pmin(rnorm(n,mean = 40+policy_year, sd = 5),60),25),0),\r\n# sum_assured, age and occupation are correlated; sum assured normally distributed around some mean (dependent on age rounded to 10 and on occupation), floored at 500\r\nsum_assured = \r\n  round(\r\n    pmax(\r\n      rnorm(n,mean = (round(age,-1)*100+ 1000) * \r\n      case_when(occupation %in% c(\"white\",\"prof\") ~ 1.5, occupation %in% c(\"sed\") ~ 1.3 , TRUE ~ 1), \r\n      sd = 2000),500),\r\n      0)\r\n  )\r\n# generate 3 years of exposure for the 200k policies => assume no lapses or new business\r\ndf2 <- df %>% mutate(cal_year=cal_year+1,policy_year=policy_year+1,age=age+1)\r\ndf3 <- df2 %>% mutate(cal_year=cal_year+1,policy_year=policy_year+1,age=age+1)\r\ndf <- rbind(df,df2,df3)\r\n\r\n\r\n\r\nExpected claim rate\r\nSet p values from which to simulate claims. The crude p values below were derived from the Society of Actuaries Analysis of USA Individual Disability Claim Incidence Experience from 2006 to 2014 (SOA 2019), with some allowance for Australian industry differentials (Ian Welch 2020).\r\n\r\n\r\nShow code\r\n\r\n# by cause, age and sex, based upon polynomials fitted to crude actual rates\r\n# sickness\r\nf_sick_age_m <- function(age) {-0.0000003*age^3 + 0.000047*age^2 - 0.00203*age + 0.02715}\r\nf_sick_age_f <- function(age) {-0.0000002*age^3 + 0.000026*age^2 - 0.00107*age + 0.01550}            \r\nf_sick_age_u <- function(age) {f_sick_age_f(age)*1.2}\r\nf_sick_age   <- function(age,sex) {case_when(sex == \"m\" ~ f_sick_age_m(age), sex == \"f\" ~ f_sick_age_f(age), sex == \"u\" ~ f_sick_age_u(age))}\r\n\r\n# accident\r\nf_acc_age_m <- function(age) {-0.00000002*age^3 + 0.000004*age^2 - 0.00020*age + 0.00340}\r\nf_acc_age_f <- function(age) {-0.00000004*age^3 + 0.000007*age^2 - 0.00027*age + 0.00374}            \r\nf_acc_age_u <- function(age) {f_sick_age_f(age)*1.2}\r\nf_acc_age   <- function(age,sex) {case_when(sex == \"m\" ~ f_acc_age_m(age), sex == \"f\" ~ f_acc_age_f(age), sex == \"u\" ~ f_acc_age_u(age))}\r\n\r\n# smoker, wp and occ based upon ratio of crude actual rates by category\r\n# occupation adjustment informed by FSC commentary on DI incidence experience\r\nf_smoker   <- function(smoker) {case_when(smoker == \"n\" ~ 1, smoker == \"s\" ~ 1.45, smoker == \"u\" ~ 0.9)}\r\nf_wp   <- function(waiting_period) {case_when(waiting_period == \"14d\" ~ 1.4, waiting_period == \"30d\" ~ 1, waiting_period == \"90d\" ~ 0.3, waiting_period == \"720d\" ~ 0.2)}\r\nf_occ_sick   <- function(occupation) {case_when(occupation == \"prof\" ~ 1, occupation == \"sed\" ~ 1, occupation == \"techn\" ~ 1, occupation == \"blue\" ~ 1, occupation == \"white\" ~ 1)}\r\nf_occ_acc   <- function(occupation) {case_when(occupation == \"prof\" ~ 1, occupation == \"sed\" ~ 1, occupation == \"techn\" ~ 4.5, occupation == \"blue\" ~ 4.5, occupation == \"white\" ~ 1)}\r\n\r\n# anecdotal allowance for higher rates at larger policy size and for older policies\r\nf_sa_sick <- function(sum_assured) {case_when(sum_assured<=6000 ~ 1, sum_assured>6000 & sum_assured<=10000 ~ 1.1, sum_assured>10000 ~ 1.3)}\r\nf_sa_acc <- function(sum_assured) {case_when(sum_assured<=6000 ~ 1, sum_assured>6000 & sum_assured<=10000 ~ 1, sum_assured>10000 ~ 1)}\r\nf_pol_yr_sick <- function(policy_year) {case_when(policy_year<=5 ~ 1, policy_year>5 & policy_year<=10 ~ 1.1, policy_year>10 ~ 1.3)}\r\nf_pol_yr_acc <- function(policy_year) {case_when(policy_year<=5 ~ 1, policy_year>5 & policy_year<=10 ~ 1, policy_year>10 ~ 1)}\r\n\r\n\r\n\r\nExpected claims\r\nAdd the crude p values to the data and simulate 1 draw from a binomial with prob = p for each record. Gives us a vector of claim/no-claim for each policy. Some simplifying assumptions like independence of sample across years for each policy and independence of accident and sickness incidences.\r\n\r\n\r\nShow code\r\n\r\n# add crude expected\r\ndf$inc_sick_expected=f_sick_age(df$age,df$sex)*f_smoker(df$smoker)*f_wp(df$waiting_period)*f_occ_sick(df$occupation)*f_sa_sick(df$sum_assured)*f_pol_yr_sick(df$policy_year)\r\ndf$inc_acc_expected=f_acc_age(df$age,df$sex)*f_smoker(df$smoker)*f_wp(df$waiting_period)*f_occ_acc(df$occupation)*f_sa_acc(df$sum_assured)*f_pol_yr_acc(df$policy_year)\r\n# add prediction\r\ndf$inc_count_sick = sapply(df$inc_sick_expected,function(z){rbinom(1,1,z)})\r\ndf$inc_count_acc = sapply(df$inc_acc_expected,function(z){rbinom(1,1,z)})*(1-df$inc_count_sick)\r\ndf$inc_count_tot = df$inc_count_sick + df$inc_count_acc\r\n# add amounts prediction\r\ndf$inc_amount_sick = df$inc_count_sick * df$sum_assured\r\ndf$inc_amount_acc =  df$inc_count_acc * df$sum_assured\r\ndf$inc_amount_tot =  df$inc_count_tot * df$sum_assured\r\n\r\n\r\n\r\nGrouped data\r\nThe data generated above are records for each individual policy, however data like this is often grouped as it is easier to store and computation is easier (Piet de Jong, Gillian Z. Heller 2008, p49, 105). Later we will consider the differences between a model on ungrouped vs grouped data.\r\n\r\n\r\nShow code\r\n\r\n# group data (see section on data manipulation below)\r\ndf_grp <- df %>% group_by(cal_year, policy_year, sex, smoker, benefit_period, waiting_period, occupation, age) %>% \r\nsummarise(sum_assured=sum(sum_assured),inc_count_sick_exp=sum(inc_sick_expected),inc_count_acc_exp=sum(inc_acc_expected),        inc_count_sick=sum(inc_count_sick),inc_count_acc=sum(inc_count_acc),inc_count_tot=sum(inc_count_tot),inc_amount_sick=sum(inc_amount_sick),inc_amount_acc=sum(inc_amount_acc),inc_amount_tot=sum(inc_amount_tot), exposure=n(),.groups = 'drop') \r\n\r\n\r\n\r\nCheck that the exposure for the grouped data is the same as the total on ungrouped:\r\n\r\n\r\nShow code\r\n\r\n# check count - same as total row count of the main df\r\nsum(df_grp$exposure)\r\n\r\n\r\n[1] 600000\r\n\r\nAnd that the number of rows of data are significantly lower:\r\n\r\n\r\nShow code\r\n\r\n# number of rows of the grouped data is significantly lower\r\nnrow(df_grp)\r\n\r\n\r\n[1] 109590\r\n\r\nData Exploration\r\nThe sections below rely heavily upon the dplyr package.\r\nData structure\r\nLooking at the metadata for the data frame and a sample of the contents.\r\n\r\n\r\nShow code\r\n\r\n# glimpse() or str() returns detail on the structure of the data frame. Our data consists of 600k rows and 15 columns. The columns are policy ID, several explanatory variables like sex and smoker, expected counts of claim (inc_sick_expected and inc_acc_expected) and actual counts of claim (inc_count_sick/acc/tot).\r\nglimpse(df)\r\n\r\n\r\nRows: 600,000\r\nColumns: 18\r\n$ id                <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,~\r\n$ cal_year          <dbl> 2018, 2018, 2018, 2018, 2018, 2018, 2018, ~\r\n$ policy_year       <dbl> 4, 5, 5, 3, 8, 6, 6, 6, 3, 5, 3, 4, 7, 4, ~\r\n$ sex               <chr> \"m\", \"f\", \"m\", \"m\", \"f\", \"f\", \"m\", \"m\", \"m~\r\n$ smoker            <chr> \"n\", \"n\", \"n\", \"n\", \"n\", \"n\", \"n\", \"n\", \"s~\r\n$ benefit_period    <chr> \"a65\", \"5yr\", \"a65\", \"a65\", \"a65\", \"2yr\", ~\r\n$ waiting_period    <chr> \"90d\", \"30d\", \"30d\", \"30d\", \"30d\", \"14d\", ~\r\n$ occupation        <chr> \"techn\", \"blue\", \"blue\", \"prof\", \"sed\", \"p~\r\n$ age               <dbl> 41, 46, 41, 27, 53, 49, 54, 42, 43, 52, 36~\r\n$ sum_assured       <dbl> 7119, 5582, 6113, 5147, 8864, 11209, 6378,~\r\n$ inc_sick_expected <dbl> 0.000742731, 0.001828800, 0.002475770, 0.0~\r\n$ inc_acc_expected  <dbl> 0.0007365330, 0.0100735200, 0.0024551100, ~\r\n$ inc_count_sick    <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ~\r\n$ inc_count_acc     <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ~\r\n$ inc_count_tot     <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ~\r\n$ inc_amount_sick   <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ~\r\n$ inc_amount_acc    <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ~\r\n$ inc_amount_tot    <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ~\r\n\r\nShow code\r\n\r\n# head() returns the first 6 rows of the data frame. Similar to head(), sample_n() returns rows from our data frame, however these are chosen randomly. e.g. sample_n(df,5,replace=FALSE)\r\nhead(df) %>% kbl(caption = \"Sample of data\")\r\n\r\n\r\n\r\n(#tab:Data structure)Sample of data\r\n\r\n\r\nid\r\n\r\n\r\ncal_year\r\n\r\n\r\npolicy_year\r\n\r\n\r\nsex\r\n\r\n\r\nsmoker\r\n\r\n\r\nbenefit_period\r\n\r\n\r\nwaiting_period\r\n\r\n\r\noccupation\r\n\r\n\r\nage\r\n\r\n\r\nsum_assured\r\n\r\n\r\ninc_sick_expected\r\n\r\n\r\ninc_acc_expected\r\n\r\n\r\ninc_count_sick\r\n\r\n\r\ninc_count_acc\r\n\r\n\r\ninc_count_tot\r\n\r\n\r\ninc_amount_sick\r\n\r\n\r\ninc_amount_acc\r\n\r\n\r\ninc_amount_tot\r\n\r\n\r\n1\r\n\r\n\r\n2018\r\n\r\n\r\n4\r\n\r\n\r\nm\r\n\r\n\r\nn\r\n\r\n\r\na65\r\n\r\n\r\n90d\r\n\r\n\r\ntechn\r\n\r\n\r\n41\r\n\r\n\r\n7119\r\n\r\n\r\n0.0007427\r\n\r\n\r\n0.0007365\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n2\r\n\r\n\r\n2018\r\n\r\n\r\n5\r\n\r\n\r\nf\r\n\r\n\r\nn\r\n\r\n\r\n5yr\r\n\r\n\r\n30d\r\n\r\n\r\nblue\r\n\r\n\r\n46\r\n\r\n\r\n5582\r\n\r\n\r\n0.0018288\r\n\r\n\r\n0.0100735\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n3\r\n\r\n\r\n2018\r\n\r\n\r\n5\r\n\r\n\r\nm\r\n\r\n\r\nn\r\n\r\n\r\na65\r\n\r\n\r\n30d\r\n\r\n\r\nblue\r\n\r\n\r\n41\r\n\r\n\r\n6113\r\n\r\n\r\n0.0024758\r\n\r\n\r\n0.0024551\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n4\r\n\r\n\r\n2018\r\n\r\n\r\n3\r\n\r\n\r\nm\r\n\r\n\r\nn\r\n\r\n\r\na65\r\n\r\n\r\n30d\r\n\r\n\r\nprof\r\n\r\n\r\n27\r\n\r\n\r\n5147\r\n\r\n\r\n0.0006981\r\n\r\n\r\n0.0005223\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n5\r\n\r\n\r\n2018\r\n\r\n\r\n8\r\n\r\n\r\nf\r\n\r\n\r\nn\r\n\r\n\r\na65\r\n\r\n\r\n30d\r\n\r\n\r\nsed\r\n\r\n\r\n53\r\n\r\n\r\n8864\r\n\r\n\r\n0.0024788\r\n\r\n\r\n0.0031379\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n6\r\n\r\n\r\n2018\r\n\r\n\r\n6\r\n\r\n\r\nf\r\n\r\n\r\nn\r\n\r\n\r\n2yr\r\n\r\n\r\n14d\r\n\r\n\r\nprof\r\n\r\n\r\n49\r\n\r\n\r\n11209\r\n\r\n\r\n0.0039363\r\n\r\n\r\n0.0036555\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\nShow code\r\n\r\n# class() returns the class of a column.\r\nclass(df$benefit_period)\r\n\r\n\r\n[1] \"character\"\r\n\r\nFactors\r\nFrom the above you’ll note that the categorical columns are stored as characters. Factorising these makes them easier to work with in our models e.g. for BP factorise a65|2yr|5yr as 1|2|3. Factors are stored as integers and have labels that tell us what they are, they can be ordered and are useful for statistical analysis.\r\ntable() returns a table of counts at each combination of column values. prop.table() converts these to a proportion. For example, applying this to the column “sex” shows us that ~75% of our data is “m” and that the other data are either “f” or “u” (unknown).\r\n\r\n\r\nShow code\r\n\r\ntable(df$sex)\r\n\r\n\r\n\r\n     f      m      u \r\n120951 449487  29562 \r\n\r\nShow code\r\n\r\nprop.table(table(df$sex))\r\n\r\n\r\n\r\n       f        m        u \r\n0.201585 0.749145 0.049270 \r\n\r\nWe can then convert the columns to factors based upon the values of the column and ordering by frequency. Base level should be chosen such that it has sufficient observations for an intercept to be computed meaningfully.\r\n\r\n\r\nShow code\r\n\r\ndf$sex <- factor(df$sex, levels = c(\"m\",\"f\",\"u\"))\r\ndf$smoker <- factor(df$smoker, levels = c(\"n\",\"s\",\"u\"))\r\ndf$benefit_period <- factor(df$benefit_period, levels = c(\"a65\",\"2yr\",\"5yr\"))\r\ndf$waiting_period <- factor(df$waiting_period, labels = c(\"30d\",\"14d\",\"720d\",\"90d\"))\r\ndf$occupation <- factor(df$occupation, labels = c(\"prof\", \"sed\",\"techn\",\"white\",\"blue\"))\r\n\r\n# do the same for the grouped data\r\ndf_grp$sex <- factor(df_grp$sex, levels = c(\"m\",\"f\",\"u\"))\r\ndf_grp$smoker <- factor(df_grp$smoker, levels = c(\"n\",\"s\",\"u\"))\r\ndf_grp$benefit_period <- factor(df_grp$benefit_period, levels = c(\"a65\",\"2yr\",\"5yr\"))\r\ndf_grp$waiting_period <- factor(df_grp$waiting_period, labels = c(\"30d\",\"14d\",\"720d\",\"90d\"))\r\ndf_grp$occupation <- factor(df_grp$occupation, labels = c(\"prof\", \"sed\",\"techn\",\"white\",\"blue\"))\r\n\r\n\r\n\r\nIf the column is already a factor, you can extract the levels to show what order they will be used in our models\r\n\r\n\r\nShow code\r\n\r\nlevels(df$sex)\r\n\r\n\r\n[1] \"m\" \"f\" \"u\"\r\n\r\nSelection methods\r\ntable() is a method of summarizing data, returning a count at each combination of values in a column. sample() and sample_n() are other examples of selection methods. This section (not exhaustive) looks at a few more selection methods in dplyr.\r\n\r\n\r\nShow code\r\n\r\n# data subsets:  e.g. select from df where age <25 or >60 \r\nsubset(df, age <25 | age > 60) \r\n#  dropping columns:\r\n    # exclude columns\r\n    mycols <- names(df) %in% c(\"cal_year\", \"smoker\")\r\n    new_df <- df[!mycols]\r\n    # exclude 3rd and 5th column\r\n    new_df <- df[c(-3,-5)]\r\n    # delete columns from new_df\r\n    new_df$pol_id <- NULL\r\n#  keeping columns: \r\n    # select variables by col name\r\n    mycols <- names(df) %in% c(\"cal_year\", \"smoker\")\r\n    new_df <- df[!mycols]\r\n    # select 1st and 5th to 7th variables\r\n    new_df <- df[c(1,5:7)]\r\n\r\n\r\n\r\nManipulation methods\r\nWe might want to modify our data frame to prepare it for fitting our models. The section below looks at a few simple data manipulations. Here we also introduce the infix operator (%>%); this operator passes the argument to the left of it over to the code on the right, so df %>% “operation” passes the data frame “df” over to the operation on the right.\r\n\r\n\r\nShow code\r\n\r\n# create a copy of the dataframe to work from\r\n  new_df <- df\r\n# simple manipulations\r\n  # select as in the selection methods section, but using infix\r\n  new_df %>% select(id, age) # or a range using select(1:5) or select(contains(\"sick\")) or select(starts_with(\"inc\")); others e.g. ends_with(), last_col(), select(-age)\r\n  # replace values in a column\r\n  replace(new_df$sex,new_df$sex==\"u\",\"m\") # no infix in base r\r\n  # Rename, id to pol_id\r\n  new_df %>% rename(pol_id = id)  #or (reversing the renaming)\r\n  new_df %>% select(pol_id = id)  \r\n  # alter data\r\n  new_df <- new_df %>% mutate(inc_tot_expected = inc_acc_expected + inc_sick_expected) # need to assign the output back to the data frame\r\n  # transmute - select and mutate simultaneously \r\n  new_df2 <- new_df %>% transmute(id, age, birth_year = cal_year - age)\r\n  # sort\r\n  new_df %>% arrange(desc(age))\r\n  # filter\r\n  new_df %>% filter(benefit_period == \"a65\", age <65) # or\r\n  new_df %>% filter(benefit_period %in% c(\"a65\",\"5yr\"))\r\n# aggregations\r\n  # group by, also ungroup()\r\n  new_df %>% group_by(sex) %>% # can add a mutate to group by which will aggregate only to the level specified in the group_by e.g. \r\n  mutate(sa_by_sex = sum(sum_assured)) # adds a new column with the total sum assured by sex.\r\n  # after doing this, ungroup() in order to apply future operations to all records individually\r\n  # count, sorting by most frequent and weighting by another column\r\n  new_df %>% count(sex, wt= sum_assured, sort=TRUE)  # counts the number of entries for each value of sex, weighted by sum assured\r\n  # summarize takes many observations and turns them into one observation. mean(), median(), min(), max(), and n() for the size of the group\r\n  new_df %>% summarize(total = sum(sum_assured), min_age = min(age), max_age = max(age), max(inc_tot_expected)) \r\n  new_df %>% group_by(sex) %>% summarise(n = n())\r\n  table(new_df$sex) # returns count by sex; no infix in base r\r\n# outliers\r\n  new_df %>% top_n(10, inc_tot_expected) # also operates on grouped table - returns top n per group\r\n# window functions\r\n  # lag - offset vector by 1 e.g. v <- c(1,3,6,14); so - lag(v) = NA 1 3 6\r\n  new_df %>% arrange(id,age) %>% mutate(ifelse(id==lag(id),age - lag(age),1))\r\n\r\n\r\n\r\nMissing data\r\nBy default, the regression model will exclude any observation with missing values on its predictors. Missing values can be treated as a separate category for categorical data. For missing numeric data, imputation is a potential solution. In the example below we replace missing age with an average and add an indicator to the data to flag records that have been imputed.\r\n\r\n\r\nShow code\r\n\r\n# find the average age among non-missing values\r\nsummary(df$age)\r\n# impute missing age values with the mean age\r\ndf$imputed_age <- ifelse(is.na(df$age)==TRUE,round(mean(df$age, na.rm=TRUE),2),df$age)\r\n# create missing value indicator for age\r\ndf$missing_age <- ifelse(is.na(df$age)==TRUE,1,0)\r\n\r\n\r\n\r\nReview exposure data\r\nThe tables and graphs that follow look at:\r\nthe mix of business over rating factors using some of the selection methods described: These are all consistent with the simulation specification.\r\nthe correlation of ordered numerical rating factors: age and sum assured as well as age and policy year are positively correlated.\r\nData might need to be transformed in order to make the data more suitable to the assumptions within the model. Not considered here.\r\n\r\n\r\nShow code\r\n\r\n# MASS package interferes with pairs() and cor() and others\r\nrequire(arm) # package\r\ndetach(package:arm)\r\ndetach(package:msme)\r\ndetach(package:MASS)\r\n\r\n\r\n\r\nLook at distribution by single rating factors.\r\n\r\n\r\nShow code\r\n\r\ndf %>% count(benefit_period, wt = NULL, sort = TRUE) %>% mutate(freq = percent(round(n / sum(n),2))) %>% format(n, big.mark=\",\") %>% kbl(caption = \"Benefit period mix\")\r\n\r\n\r\n\r\n(#tab:Review exposure data - proportions)Benefit period mix\r\n\r\n\r\nbenefit_period\r\n\r\n\r\nn\r\n\r\n\r\nfreq\r\n\r\n\r\na65\r\n\r\n\r\n456,552\r\n\r\n\r\n76%\r\n\r\n\r\n2yr\r\n\r\n\r\n72,159\r\n\r\n\r\n12%\r\n\r\n\r\n5yr\r\n\r\n\r\n71,289\r\n\r\n\r\n12%\r\n\r\n\r\nShow code\r\n\r\ndf %>% count(waiting_period, wt = NULL, sort = TRUE) %>% mutate(freq = percent(round(n / sum(n),2))) %>% format(n, big.mark=\",\")  %>% kbl(caption = \"Waiting period mix\")\r\n\r\n\r\n\r\n(#tab:Review exposure data - proportions)Waiting period mix\r\n\r\n\r\nwaiting_period\r\n\r\n\r\nn\r\n\r\n\r\nfreq\r\n\r\n\r\n14d\r\n\r\n\r\n420,009\r\n\r\n\r\n70.0%\r\n\r\n\r\n90d\r\n\r\n\r\n90,717\r\n\r\n\r\n15.0%\r\n\r\n\r\n720d\r\n\r\n\r\n65,814\r\n\r\n\r\n11.0%\r\n\r\n\r\n30d\r\n\r\n\r\n23,460\r\n\r\n\r\n4.0%\r\n\r\n\r\nShow code\r\n\r\ndf %>% count(occupation, wt = NULL, sort = TRUE) %>% mutate(freq = percent(round(n / sum(n),2))) %>% format(n, big.mark=\",\") %>% kbl(caption = \"Occupation mix\")\r\n\r\n\r\n\r\n(#tab:Review exposure data - proportions)Occupation mix\r\n\r\n\r\noccupation\r\n\r\n\r\nn\r\n\r\n\r\nfreq\r\n\r\n\r\nsed\r\n\r\n\r\n240,348\r\n\r\n\r\n40%\r\n\r\n\r\ntechn\r\n\r\n\r\n120,717\r\n\r\n\r\n20%\r\n\r\n\r\nwhite\r\n\r\n\r\n119,847\r\n\r\n\r\n20%\r\n\r\n\r\nblue\r\n\r\n\r\n59,613\r\n\r\n\r\n10%\r\n\r\n\r\nprof\r\n\r\n\r\n59,475\r\n\r\n\r\n10%\r\n\r\n\r\nConsider a histogram to show the distribution of numeric data.\r\n\r\n\r\nShow code\r\n\r\nhist(df$age, main = \"Histogram of age\", xlab = \"Age\", ylab = \"Frequency\")\r\n\r\n\r\n\r\nShow code\r\n\r\nhist(df$sum_assured, main = \"Histogram of sum assured\", xlab = \"Sum assured\", ylab = \"Frequency\")\r\n\r\n\r\n\r\nShow code\r\n\r\nhist(df$policy_year, main = \"Histogram of policy year\", xlab = \"Policy year\", ylab = \"Frequency\")\r\n\r\n\r\n\r\n\r\nConsider the correlation of ordered numeric explanatory variables.\r\n\r\n\r\nShow code\r\n\r\n# correlation of ordered numeric explanatory variables\r\n#  pairs() gives correlation matrix and plots; test on a random sample from our data\r\ndf_sample <- sample_n(df,10000,replace=FALSE) \r\ndf_sample %>% select(age,policy_year,sum_assured) %>% pairs\r\n\r\n\r\n\r\nShow code\r\n\r\n# or cor() to return just the correlation matrix\r\ncor <-df_sample %>% select(age,policy_year,sum_assured) %>% cor\r\ncor\r\n\r\n\r\n                  age policy_year sum_assured\r\nage         1.0000000   0.4333434   0.2937906\r\npolicy_year 0.4333434   1.0000000   0.1249400\r\nsum_assured 0.2937906   0.1249400   1.0000000\r\n\r\nShow code\r\n\r\n# corrplot() is an alternative to visualise a correlation matrix\r\ncorrplot(cor, \r\n  addCoef.col = \"black\", # add coefficient of correlation\r\n  method=\"color\", \r\n  sig.level = 0.01, insig = \"blank\", \r\n  tl.col=\"black\", # tl stands for text label\r\n  tl.srt=45\r\n  ) \r\n\r\n\r\n\r\n\r\nggpairs() similarly shows correlations for ordered numeric data as well as other summary stats:\r\n\r\n\r\nShow code\r\n\r\n#ggpairs() similarly shows correlations for ordered numeric data as well as other summary stats\r\ndf_sample %>% select(age,policy_year,sum_assured,sex, smoker) %>% \r\nggpairs(columns = 1:3, aes(color = sex, alpha = 0.5),\r\n        upper = list(continuous = wrap(\"cor\", size = 2.5)),\r\n        lower = list(continuous = \"smooth\"))\r\n\r\n\r\n\r\nShow code\r\n\r\ndf_sample %>% select(age,policy_year,sum_assured,sex, smoker) %>% \r\nggpairs(columns = c(\"sum_assured\", \"smoker\"), aes(color = sex, alpha = 0.5))\r\n\r\n\r\n\r\n\r\nReview summary statistics for subsets of data.\r\n\r\n\r\nShow code\r\n\r\nhead(aggregate(df$sum_assured~df$age,data=df,mean))\r\n\r\n\r\n  df$age df$sum_assured\r\n1     25       3584.319\r\n2     26       4431.558\r\n3     27       4735.551\r\n4     28       5066.777\r\n5     29       5068.259\r\n6     30       5204.272\r\n\r\nData format\r\nThere are two main formats for structured data - long and wide. For regression, the structure of data informs the model structure. For counts data:\r\nthe long format corresponds to Bernoulli (claim or no claim for each observation) and allows for predictor variables by observation;\r\nthe wide format correspond to Binomial (count of claims per exposure). Wide format structures can include matrix of successes and failures or a proportion of successes and corresponding weights / number of observations/exposure for each line.\r\nThere are several tidyverse functions that can help with restructuring data, for example, convert data into wide format e.g.separate into a separate column for each value of sex:\r\n\r\n\r\nShow code\r\n\r\nhead(spread(df_sample, sex, inc_count_tot, fill = 0)) %>% kbl(caption = \"Wide data format example\")\r\n\r\n\r\n\r\n(#tab:Data format)Wide data format example\r\n\r\n\r\nid\r\n\r\n\r\ncal_year\r\n\r\n\r\npolicy_year\r\n\r\n\r\nsmoker\r\n\r\n\r\nbenefit_period\r\n\r\n\r\nwaiting_period\r\n\r\n\r\noccupation\r\n\r\n\r\nage\r\n\r\n\r\nsum_assured\r\n\r\n\r\ninc_sick_expected\r\n\r\n\r\ninc_acc_expected\r\n\r\n\r\ninc_count_sick\r\n\r\n\r\ninc_count_acc\r\n\r\n\r\ninc_amount_sick\r\n\r\n\r\ninc_amount_acc\r\n\r\n\r\ninc_amount_tot\r\n\r\n\r\nm\r\n\r\n\r\nf\r\n\r\n\r\nu\r\n\r\n\r\n186530\r\n\r\n\r\n2019\r\n\r\n\r\n6\r\n\r\n\r\nn\r\n\r\n\r\na65\r\n\r\n\r\n14d\r\n\r\n\r\nsed\r\n\r\n\r\n45\r\n\r\n\r\n7850\r\n\r\n\r\n0.0044014\r\n\r\n\r\n0.0006775\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n89986\r\n\r\n\r\n2020\r\n\r\n\r\n4\r\n\r\n\r\nn\r\n\r\n\r\na65\r\n\r\n\r\n14d\r\n\r\n\r\ntechn\r\n\r\n\r\n48\r\n\r\n\r\n8359\r\n\r\n\r\n0.0053024\r\n\r\n\r\n0.0008042\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n12688\r\n\r\n\r\n2019\r\n\r\n\r\n6\r\n\r\n\r\nn\r\n\r\n\r\n5yr\r\n\r\n\r\n14d\r\n\r\n\r\nwhite\r\n\r\n\r\n48\r\n\r\n\r\n9391\r\n\r\n\r\n0.0058327\r\n\r\n\r\n0.0036187\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n16974\r\n\r\n\r\n2019\r\n\r\n\r\n6\r\n\r\n\r\ns\r\n\r\n\r\na65\r\n\r\n\r\n14d\r\n\r\n\r\nwhite\r\n\r\n\r\n49\r\n\r\n\r\n2687\r\n\r\n\r\n0.0083455\r\n\r\n\r\n0.0055529\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n70839\r\n\r\n\r\n2020\r\n\r\n\r\n7\r\n\r\n\r\nn\r\n\r\n\r\na65\r\n\r\n\r\n14d\r\n\r\n\r\nprof\r\n\r\n\r\n41\r\n\r\n\r\n3597\r\n\r\n\r\n0.0024758\r\n\r\n\r\n0.0024551\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n120872\r\n\r\n\r\n2018\r\n\r\n\r\n4\r\n\r\n\r\nn\r\n\r\n\r\na65\r\n\r\n\r\n14d\r\n\r\n\r\nprof\r\n\r\n\r\n46\r\n\r\n\r\n5177\r\n\r\n\r\n0.0040212\r\n\r\n\r\n0.0032278\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\n0\r\n\r\n\r\ngather() converts data into long format; # also pivot_longer() and pivot_wider().\r\nVisualisation methods\r\nIntroduction to ggplot\r\nThis section sets out some simple visualisation methods using ggplot(). ggplot() Initializes a ggplot object. It can be used to declare the input data frame for a graphic and to specify the set of plot aesthetics intended to be common throughout all subsequent layers unless specifically overridden (pkgdown, n.d.a). The form of ggplot is:\r\nggplot(data = df, mapping = aes(x,y, other aesthetics), …)\r\nExamples below use ggplot to explore the exposure data.\r\n\r\n\r\nShow code\r\n\r\n# data argument passes the data frame to be visualised\r\n# mapping argument defines a list of aesthetics for the visualisation - all subsequent layers use those unless overridden\r\n# typically, the dependent variable is mapped onto the the y-axis and the independent variable is mapped onto the x-axis.\r\nggplot(data=df_sample, mapping=aes(x=age, y=sum_assured)) + # the '+' adds the layer below\r\n# add subsequent visualisation layers, e.g. geom_point() for scatterplot\r\ngeom_point() +\r\n# add a layer to change axis labels\r\n# could add a layer to specify axis limits with ylim() and xlim() \r\nlabs(x=\"Age\", y=\"Sum assured\", title = \"Sum Assured by age\")\r\n\r\n\r\n\r\n\r\nLayers\r\nThe aesthetics input has a number of different options, for example x and y (axes), colour, size, fill, labels, alpha (transparency), shape, line type/ width. You can change the aesthetics of each layer or default to the base layer. You can change the general look and feel of charts with a themes layer e.g. colour palette (see more in the next section).\r\nYou can add more layers to the base plot, for example\r\nGeometries (geom_), for example\r\npoint - scatterplot,\r\nline,\r\nhistogram,\r\nbar/ column,\r\nboxplot,\r\ndensity,\r\njitter - adds random noise to separate points,\r\ncount - counts the number of observations at each location, then maps the count to point area,\r\nabline - adds a reference line - vertical, horizontal or diagonal,\r\ncurve - adds a curved line to the chart between specified points,\r\ntext - add a text layer to label data points.\r\n\r\nStatistics (stat_)\r\nsmooth (curve fitted to the data),\r\nbin (e.g. for histogram).\r\n\r\nA note on overlapping points: these can be adjusted for by adding noise and transparency to your points:\r\nwithin an existing geom e.g. geom_point(position=\"*“) with options including: identity (default = position is as per data), dodge (dodge overlapping objects side-to-side), jitter (random noise), jitterdodge, and nudge (nudge points a fixed distance) e.g. geom_bar(position =”dodge\") or geom_bar(position=position_dodge(width=0.2)).\r\nor use geom_* with arguments e.g. geom_jitter(alpha = 0.2, shape=1). Shape choice might help, shape = 1 gives hollow circles.\r\nOr alternatively count overlapping points with geom_count().\r\nA full list of layers is available here.\r\n\r\n\r\nShow code\r\n\r\nggplot(data=df_sample, aes(x=age, y=sum_assured)) +\r\ngeom_point() + \r\n# separate overlapping points\r\ngeom_jitter(alpha = 0.2, width = 0.2) +\r\n# add a smoothing line\r\ngeom_smooth(method = \"glm\", se=FALSE) \r\n\r\n\r\n\r\n\r\nThemes\r\nYou can add a themes layer to your graph (pkgdown, n.d.b), for example\r\ntheme_gray() |Gray background and white grid lines.\r\ntheme_bw() |White background and gray grid lines.\r\ntheme_linedraw() |A theme with only black lines of various widths on white backgrounds.\r\ntheme_light() |A theme similar to theme_linedraw() but with light grey lines and axes, to direct more attention towards the data.\r\ntheme_dark() |Similar to theme_light() but with a dark background.\r\nOthers |e.g. theme_minimal() and theme_classic()\r\nOther packages like ggthemes carry many more options. Example of added themes layer below. See also these examples these examples from ggthemes.\r\n\r\n\r\nShow code\r\n\r\n# add an occupation group to the data\r\ndf_sample <- df_sample %>% mutate(occ_group = factor(case_when(occupation %in% c(\"white\",\"prof\",\"sed\") ~ \"WC\", TRUE ~ \"BC\")))\r\n\r\n# vary colour by occupation\r\nggplot(data=df_sample, aes(x=age, y=sum_assured, color=occ_group)) +\r\n# jitter and fit a smoothed line as below\r\ngeom_jitter(alpha = 0.2, width = 0.2) +\r\ngeom_smooth(method = \"glm\", se=FALSE) +\r\n# add labels\r\nlabs(x=\"Age\", y=\"Sum assured\", title = \"Sum Assured by age\") +\r\n# adding theme and colour palette layers\r\ntheme_pander() + scale_color_gdocs()\r\n\r\n\r\n\r\n\r\n3D visualisations with plotly\r\nggplot does not cater to 3D visualisations, but this can be done through plotly simply.\r\n\r\n\r\nShow code\r\n\r\nplot_base <- plot_ly(data=df_sample, z= ~sum_assured, x= ~age, y=~policy_year, opacity=0.6) %>%\r\nadd_markers(color = ~occ_group,colors= c(\"blue\", \"red\"), marker=list(size=2)) \r\n# show graph\r\nplot_base\r\n\r\n\r\npreservee08fab204ef70f67\r\n\r\nWe can add a modeled outcome to the 3D chart. For detail on the model fit, see later sections.\r\n\r\n\r\nShow code\r\n\r\n# to add a plane we need to define the points on the plane. To do that, we first create a grid of x and y values, where x and y are defined as earlier.\r\nx_grid <- seq(from = min(df_sample$age), to = max(df_sample$age), length = 50)\r\ny_grid <- seq(from = min(df_sample$policy_year), to = max(df_sample$policy_year), length = 50)\r\n\r\n# create a simple model and extract the coefficient estimates\r\ncoeff_est <- glm(sum_assured ~ age + policy_year + occ_group,family=\"gaussian\",data=df_sample) %>% coef()\r\n# extract fitted values for z - here we want fitted values for BC and WC separately, use levels to determine how the model orders the factor occ_group\r\nfitted_values_BC <- crossing(y_grid, x_grid) %>% mutate(z_grid = coeff_est[1] + coeff_est[2]*x_grid + coeff_est[3]*y_grid)\r\nfitted_values_WC <- crossing(y_grid, x_grid) %>% mutate(z_grid = coeff_est[1] + coeff_est[2]*x_grid + coeff_est[3]*y_grid + coeff_est[4])\r\n# convert to matrix\r\nz_grid_BC <- fitted_values_BC %>% pull(z_grid) %>% matrix(nrow = length(x_grid)) %>% t()\r\nz_grid_WC <- fitted_values_WC %>% pull(z_grid) %>% matrix(nrow = length(x_grid)) %>% t()\r\n\r\n# define solid colours for the two planes/ surfaces\r\ncolorscale_BC = list(c(0, 1), c(\"red\", \"red\"))\r\ncolorscale_WC = list(c(0, 1), c(\"blue\", \"blue\"))\r\n\r\n# use plot base created above, add a surface for BC sum assureds and WC sum assureds\r\nplot_base %>%\r\n    add_surface(x = x_grid, y = y_grid, z = z_grid_BC, showscale=FALSE, colorscale=colorscale_BC) %>%\r\n    add_surface(x = x_grid, y = y_grid, z = z_grid_WC, showscale=FALSE, colorscale=colorscale_WC) %>% \r\n    # filtering sum assured on a narrower range\r\n    layout(scene = list(zaxis = list(range=c(4000,12000))))\r\n\r\n\r\npreserve5b453c8487b59283\r\n\r\nReview claim data\r\nConsider claim vs no claim. should be close to nil overlapping clams. actual claim rate is ~0.003-0.005.\r\n\r\n\r\nShow code\r\n\r\ndf %>% select(inc_count_acc,inc_count_sick) %>% table()\r\n\r\n\r\n             inc_count_sick\r\ninc_count_acc      0      1\r\n            0 596750   2087\r\n            1   1163      0\r\n\r\nPlotting claim vs no claim by age and sex:\r\n\r\n\r\nShow code\r\n\r\n# use ggplot to plot inc_count_sick by age and sex; using df_sample from earlier\r\n# clearly all of the points are going to be at 0 or 1 and will overlap at each age -> not useful.\r\ndf_sample %>% ggplot(aes(x=age,y=inc_count_sick,color=sex)) +\r\ngeom_point() +\r\ntheme_pander() + scale_color_gdocs()\r\n\r\n\r\n\r\n\r\nAs above but add some random noise around the points to separate them:\r\n\r\n\r\nShow code\r\n\r\ndf_sample %>% ggplot(aes(x=age,y=inc_count_sick,color=sex)) +\r\ngeom_point(position=position_jitter(height=0.1)) +\r\ntheme_pander() + scale_color_gdocs()\r\n\r\n\r\n\r\n\r\nAs above but excluding unknown sex and adding a smoothing line:\r\n\r\n\r\nShow code\r\n\r\n# as above but excluding unknown sex (as there are very few claims observed for that group) and adding a smoothing line (setting method as glm)\r\n# because the claim rate is so low, the smoothed line is very close to zero and so not a particularly useful visualisation.\r\ndf_sample %>% filter(sex != \"u\") %>% ggplot(aes(x=age,y=inc_count_sick,color=sex)) +\r\ngeom_point(position=position_jitter(height=0.1)) + \r\ngeom_smooth(method=\"glm\", method.args = list(family = \"binomial\")) + # or list(family = binomial(link='logit')\r\ntheme_pander() + scale_color_gdocs()\r\n\r\n\r\n\r\n\r\nLooking at total count of claim rather than just sickness shows a slight trend by age:\r\n\r\n\r\nShow code\r\n\r\ndf_sample %>% filter(sex != \"u\") %>% ggplot(aes(x=age,y=inc_count_tot,color=sex)) +\r\ngeom_point(position=position_jitter(height=0.1)) + \r\ngeom_smooth(method=\"glm\", method.args = list(family = \"binomial\")) + # or list(family = binomial(link='logit')\r\ntheme_pander() + scale_color_gdocs()\r\n\r\n\r\n\r\n\r\nConsider claim rate:\r\n\r\n\r\nShow code\r\n\r\n# given the actual count of claims is so low, it might be more useful to consider the claim rate\r\n# use the manipulation methods from earlier to get claim rates by age and sex for accident and sickness; filter out unknown sex and age with low exposure\r\n# this shows a clear trend by age for males and females\r\ndf_grouped <- df %>% filter(sex != \"u\", between(age, 30,60)) %>% group_by(age,sex) %>% summarise(total_sick=sum(inc_count_sick),total_acc=sum(inc_count_acc), exposure=n(),.groups = 'drop') %>% \r\nmutate(sick_rate = total_sick/exposure, acc_rate = total_acc/exposure)\r\n# used ggplot to graph the results\r\ndf_grouped %>%\r\nggplot(aes(x=age,y=sick_rate,color=sex)) +\r\ngeom_point() +\r\ngeom_line() +\r\n# add a smoothing line\r\ngeom_smooth(method = 'glm',se=FALSE) +\r\n# add labels and themes\r\nlabs(x=\"Age\", y=\"sick rate\", title = \"Sickness rate by age\") +\r\ntheme_pander() + scale_color_gdocs()\r\n\r\n\r\n\r\n\r\nWe can split the graph above into a few tiles to show the rates by other explanatory variables like occupation using facet_wrap; see also “facet_grid.” Can use grid.arrange(plot_1,plot_2, plot_3) from the pdp package to arrange unrelated pplot items.\r\n\r\n\r\nShow code\r\n\r\n# as above, but adding occupation\r\ndf_grouped <- df %>% filter(sex != \"u\", between(age, 30,60)) %>% group_by(age,sex,occupation) %>% summarise(total_sick=sum(inc_count_sick),total_acc=sum(inc_count_acc), exposure=n(),.groups = 'drop') %>% mutate(sick_rate = total_sick/exposure, acc_rate = total_acc/exposure) \r\n\r\ndf_grouped %>%\r\n# used ggplot to graph the results\r\nggplot(aes(x=age,y=sick_rate,color=sex)) +\r\ngeom_point() +\r\ngeom_line() +\r\n# add a smoothing line\r\ngeom_smooth(method = 'glm',se=FALSE) +\r\nlabs(x=\"Age\", y=\"sick rate\", title = \"Sickness rate by age, occupation\") +\r\ntheme_pander() + scale_color_gdocs() +\r\nfacet_wrap(~occupation, ncol=2, nrow=3)\r\n\r\n\r\n\r\n\r\nConsider sickness rate by occupation:\r\n\r\n\r\nShow code\r\n\r\ndf_grouped %>%\r\n# used ggplot to graph the results\r\nggplot(aes(x=occupation,y=sick_rate)) +\r\ngeom_boxplot(outlier.colour=\"black\", outlier.shape=16, outlier.size=2, notch=FALSE)+\r\n# add a smoothing line\r\nlabs(x=\"Age\", y=\"sick rate\", title = \"Sickness rate by age, occupation\") +\r\ntheme_pander() + scale_color_gdocs() +\r\nfacet_wrap(~sex, ncol=2, nrow=3)\r\n\r\n\r\n\r\n\r\nModel selection\r\nThe sections below provide a refresher on linear and logistic regression; some considerations for insurance data; model selection and testing model fit.\r\nSplitting data\r\nTraining vs testing data\r\nSplit data into training and testing data sets. We will used 75% of the data for training and the rest for testing.\r\n\r\n\r\nShow code\r\n\r\n# Determine the number of rows for training\r\nnrow(df)\r\n\r\n\r\n[1] 600000\r\n\r\nShow code\r\n\r\n# Create a random sample of row IDs\r\nsample_rows <- sample(nrow(df),0.75*nrow(df))\r\n# Create the training dataset\r\ndf_train <- df[sample_rows,]\r\n# Create the test dataset\r\ndf_test <- df[-sample_rows,]\r\n\r\n\r\n\r\nClass imbalance\r\nLooking at the random sample we have created, we have a significant imbalance between successes and failures.\r\n\r\n\r\nShow code\r\n\r\ndf_train %>% select(inc_count_acc,inc_count_sick) %>% table()\r\n\r\n\r\n             inc_count_sick\r\ninc_count_acc      0      1\r\n            0 447547   1574\r\n            1    879      0\r\n\r\nGenerally, “in insurance applications, the prediction of a claim or no claim on an individual policy is rarely the point of statistical modelling … The model is useful provided it explains the variability in claims behaviour, as a function of risk factors.” (Piet de Jong, Gillian Z. Heller 2008, p108–109) So, as long as we have sufficient actual claims to justify the level of predictor variables fitted to the model we should be ok.\r\nHowever, in some cases where it is important to correctly predict the binary categorical response variable, we may need to create a sample that has a roughly equal proportion of classes (of successes and failures) and then fit our model to that data. E.g. where we are looking to accurately predict fraud within banking transactions data.\r\nTo do that we need to refine our sampling method\r\nDown sampling: the majority class is randomly down sampled to be of the same size as the smaller class.\r\nUp sampling: rows from the minority class (e.g. claim) are repeatedly sampled over and over until they reaches the same size as the majority class (not claim).\r\nHybrid sampling: artificial data points are generated and are systematically added around the minority class.\r\nShowing a method for down sampling below - this is more useful for pure classification models; not that useful for insurance applications.\r\n\r\n\r\nShow code\r\n\r\n# Determine the number of rows for training\r\ndf_train_ds <- downSample(df_train,factor(df_train$inc_count_sick)) \r\ndf_train_ds %>% select(inc_count_sick) %>% table()\r\n\r\n\r\n.\r\n   0    1 \r\n1574 1574 \r\n\r\nRegression\r\nBackground\r\nLinear regression is a method of modelling the relationship between a response (dependent variable) and one or more explanatory variables (predictors; independent variables). For a data set , the relationship between y, the response/ dependent variables and the vector of x’s, the explanatory variables, is linear of the form\r\n\\(y_{i} = \\beta_{0} + \\beta_{1}x_{i1} + ... + \\beta_{p}x_{ip} + \\epsilon_{i} = \\mathbf{x}_{i}^t\\mathbf{\\beta} + \\epsilon_{i}\\), \\(i = 1,...,n\\)\r\nKey assumptions\r\nlinearity - response is some linear combination of regression coefficients and explanatory variables\r\nconstant variance (homoskedastic) - variance of errors does not depend upon explanatory variables\r\nerrors are independent - response variables uncorrelated\r\nexplanatory variables are not perfectly co-linear\r\nweak exogeneity - predictors treated as fixed / no measurement error.\r\nLinear vs logistic regression\r\nLinear: Continuous response outcome i.e. predicting a continuous response/dependent variable using a set of explanatory variables\r\nLogistic: Binary response outcome - straight line does not fit the data well. The predicted values are always between 0 and 1.\r\nFor logistic regression the log odds are linear. We can transform to odds ratios by taking the exponential of the coefficients or exp(coef(model)) - this shows the relative change to odds of the response variables, where\r\nOdds-ratio = 1, the coefficient has no effect.\r\nOdds-ratio is <1, the coefficient predicts a decreased chance of an event occurring.\r\nOdds-ratio is >1, the coefficient predicts an increased chance of an event occurring.\r\nLogistic regression for claims incidence\r\nA simple regression model is shown below. Interpreting co-efficients and other output:\r\nIntercept - global intercept and reference intercepts for each group (reference intercept: + contract (y~x); or intercept for each group: y~x-1)\r\nSlopes - other coefficients, estimates linear coefficient for continuous variable\r\nOther output\r\nPr(>|z|)/ p value is the probability coefficient result you’re seeing happened due to random variation. Commonly a p-value of .05 or less is significant.\r\nAIC and likelihood tests, useful for model comparison.\r\nResiduals sections gives summary stats of the model.\r\nCall gives the form of the model.\r\nvcov(model) gives the variance-covariance matrix for fitted model (diagonals are variance and off diagonals are covariance - 0 if all variables are orthogonal).\r\ncoef(model) returns the model coefficients.\r\n\r\n\r\nShow code\r\n\r\nmodel_1 <- glm(inc_count_sick~age,data=df_train,family=\"binomial\") # use the default link\r\nsummary(model_1)\r\n\r\n\r\n\r\nCall:\r\nglm(formula = inc_count_sick ~ age, family = \"binomial\", data = df_train)\r\n\r\nDeviance Residuals: \r\n    Min       1Q   Median       3Q      Max  \r\n-0.1719  -0.0925  -0.0764  -0.0662   3.8636  \r\n\r\nCoefficients:\r\n              Estimate Std. Error z value Pr(>|z|)    \r\n(Intercept) -10.143801   0.221246  -45.85   <2e-16 ***\r\nage           0.095743   0.004542   21.08   <2e-16 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\n(Dispersion parameter for binomial family taken to be 1)\r\n\r\n    Null deviance: 20946  on 449999  degrees of freedom\r\nResidual deviance: 20499  on 449998  degrees of freedom\r\nAIC: 20503\r\n\r\nNumber of Fisher Scoring iterations: 9\r\n\r\nChoice of link function: the link function defines the relationship of response variables to mean. It is usually sufficient to use the standard link. Section below shows a model with the link specified (results are the same as the model above).\r\n\r\n\r\nShow code\r\n\r\nmodel_2 <- glm(inc_count_sick~age,data=df_train,family=binomial(link=\"logit\")) # specify the link, logit is default for binomial\r\nsummary(model_2)\r\n\r\n\r\n\r\nAdding more response variables: When deciding on explanatory variables to model, consider the properties of the data (like correlation or colinearity).\r\n\r\n\r\nShow code\r\n\r\nmodel_3 <- glm(inc_count_sick~age+policy_year+sex+waiting_period,data=df_train,family=binomial(link=\"logit\")) \r\nsummary(model_3)\r\n\r\n\r\n\r\nCall:\r\nglm(formula = inc_count_sick ~ age + policy_year + sex + waiting_period, \r\n    family = binomial(link = \"logit\"), data = df_train)\r\n\r\nDeviance Residuals: \r\n    Min       1Q   Median       3Q      Max  \r\n-0.2363  -0.0979  -0.0751  -0.0522   4.0480  \r\n\r\nCoefficients:\r\n                    Estimate Std. Error z value Pr(>|z|)    \r\n(Intercept)        -9.507516   0.247391 -38.431  < 2e-16 ***\r\nage                 0.092744   0.005108  18.155  < 2e-16 ***\r\npolicy_year         0.012077   0.010813   1.117  0.26402    \r\nsexf               -0.723963   0.080513  -8.992  < 2e-16 ***\r\nsexu               -0.392234   0.133309  -2.942  0.00326 ** \r\nwaiting_period14d  -0.225423   0.107046  -2.106  0.03522 *  \r\nwaiting_period720d -1.831494   0.184362  -9.934  < 2e-16 ***\r\nwaiting_period90d  -1.590302   0.155151 -10.250  < 2e-16 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\n(Dispersion parameter for binomial family taken to be 1)\r\n\r\n    Null deviance: 20946  on 449999  degrees of freedom\r\nResidual deviance: 20032  on 449992  degrees of freedom\r\nAIC: 20048\r\n\r\nNumber of Fisher Scoring iterations: 9\r\n\r\nLinear regression for claims incidence\r\nChoice of response distribution: In the logistic regression example above we considered the Binomial response distribution. Other common count distributions are\r\nPoisson distribution: mean and variance are equal.\r\n\r\n\r\nShow code\r\n\r\nlower<-qpois(0.001, lambda=5)\r\nupper<-qpois(0.999, lambda=5)\r\nx<-seq(lower,upper,1)\r\n\r\ndata.frame(x, y=dpois(x, lambda=5)) %>% ggplot(mapping=aes(y=y,x=x)) + geom_col()+\r\nlabs(x=NULL, y=\"Density\", title = \"Density, poisson [dpois(x,lambda=5)]\")+\r\ntheme_pander() + scale_color_gdocs()\r\n\r\n\r\n\r\n\r\nNegative Binomial distribution: can handle Poission overdispersion (where the variance is bigger than expected).\r\n\r\n\r\nShow code\r\n\r\nlower<-qnbinom(0.001, size=2, mu=10)\r\nupper<-qnbinom(0.999, size=2, mu=10)\r\nx<-seq(lower,upper,1)\r\n\r\ndata.frame(x, y=dnbinom(x, size=2, mu=10)) %>% ggplot(mapping=aes(y=y,x=x)) + geom_col()+\r\nlabs(x=NULL, y=\"Density\", title = \"Density, negative binomial [dnbinom(x, size=2, mu=10)]\")+\r\ntheme_pander() + scale_color_gdocs()\r\n\r\n\r\n\r\n\r\nAnd amount\r\nNormal distribution\r\n\r\n\r\nShow code\r\n\r\nx<-seq(-3.5,3.5,0.1)  \r\ndata.frame(x,y=dnorm(x, mean=0, sd=1)) %>% ggplot(mapping=aes(y=y,x=x)) + geom_line()+\r\nlabs(x=NULL, y=\"Density\", title = \"Density, standard normal [dnorm(x, mean=0, sd=1)]\")+\r\ntheme_pander() + scale_color_gdocs()\r\n\r\n\r\n\r\n\r\nGamma distribution: allows for a fatter tail.\r\n\r\n\r\nShow code\r\n\r\nlower<-qgamma(0.001, shape=5, rate=3)\r\nupper<-qgamma(0.999, shape=5, rate=3)\r\nx<-seq(lower,upper,0.01)\r\n\r\ndata.frame(x,y=dgamma(x, shape=5, rate=3)) %>% ggplot(mapping=aes(y=y,x=x)) + geom_line()+\r\nlabs(x=NULL, y=\"Density\", title = \"Density, gamma [dgamma(x,shape=5, rate=3)]\")+\r\ntheme_pander() + scale_color_gdocs()\r\n\r\n\r\n\r\n\r\nAn example of an amount model, assuming a normal response distribution\r\n\r\n\r\nShow code\r\n\r\namount_model_3 <- glm(inc_amount_sick~age+sex+waiting_period,data=df_train) \r\ntidy(amount_model_3)\r\n\r\n\r\n# A tibble: 7 x 5\r\n  term               estimate std.error statistic   p.value\r\n  <chr>                 <dbl>     <dbl>     <dbl>     <dbl>\r\n1 (Intercept)          -95.8      6.89     -13.9  5.59e- 44\r\n2 age                    3.08     0.129     24.0  1.05e-126\r\n3 sexf                 -17.2      1.80      -9.56 1.24e- 21\r\n4 sexu                 -10.7      3.34      -3.19 1.41e-  3\r\n5 waiting_period14d     -6.54     3.74      -1.75 8.01e-  2\r\n6 waiting_period720d   -33.8      4.23      -7.98 1.51e- 15\r\n7 waiting_period90d    -31.7      4.08      -7.77 7.80e- 15\r\n\r\n(Un)grouped data\r\nThe models above were based upon ungrouped data, but earlier we noted that data are often grouped. In grouping data, some detail is usually lost e.g. we no longer have any true continuous categorical response variables. In this case, the columns relating to actual events (here claims) are a sum of the individual instance; more at Piet de Jong, Gillian Z. Heller (2008), p49, 105.\r\nThe example below compares a model of counts by age and sex on the grouped and shows that the derived parameters are materially similar.\r\n\r\n\r\nShow code\r\n\r\n# grouped model\r\nmodel_4 <- glm(cbind(inc_count_sick,exposure-inc_count_sick)~age+sex,data=df_grp,family=binomial(link=\"logit\")) # cbind gives a matrix of successes and failures\r\n# use tidy to visualise the modelled result\r\ntidy(model_4) %>% kbl(caption = \"Modeled result - grouped data\")\r\n\r\n\r\n\r\n(#tab:Logistic regression - grouped)Modeled result - grouped data\r\n\r\n\r\nterm\r\n\r\n\r\nestimate\r\n\r\n\r\nstd.error\r\n\r\n\r\nstatistic\r\n\r\n\r\np.value\r\n\r\n\r\n(Intercept)\r\n\r\n\r\n-9.8291510\r\n\r\n\r\n0.1917770\r\n\r\n\r\n-51.253036\r\n\r\n\r\n0.0000000\r\n\r\n\r\nage\r\n\r\n\r\n0.0918447\r\n\r\n\r\n0.0039418\r\n\r\n\r\n23.300262\r\n\r\n\r\n0.0000000\r\n\r\n\r\nsexf\r\n\r\n\r\n-0.7409699\r\n\r\n\r\n0.0702904\r\n\r\n\r\n-10.541548\r\n\r\n\r\n0.0000000\r\n\r\n\r\nsexu\r\n\r\n\r\n-0.4611341\r\n\r\n\r\n0.1188281\r\n\r\n\r\n-3.880683\r\n\r\n\r\n0.0001042\r\n\r\n\r\nShow code\r\n\r\n# ungrouped model, coefficients materially similar\r\nmodel_5 <- glm(inc_count_sick~age+sex,data=df,family=binomial(link=\"logit\")) \r\ntidy(model_5) %>% kbl(caption = \"Modeled result - ungrouped data\")\r\n\r\n\r\n\r\n(#tab:Logistic regression - grouped)Modeled result - ungrouped data\r\n\r\n\r\nterm\r\n\r\n\r\nestimate\r\n\r\n\r\nstd.error\r\n\r\n\r\nstatistic\r\n\r\n\r\np.value\r\n\r\n\r\n(Intercept)\r\n\r\n\r\n-9.8291510\r\n\r\n\r\n0.1917777\r\n\r\n\r\n-51.252824\r\n\r\n\r\n0.0000000\r\n\r\n\r\nage\r\n\r\n\r\n0.0918447\r\n\r\n\r\n0.0039418\r\n\r\n\r\n23.300170\r\n\r\n\r\n0.0000000\r\n\r\n\r\nsexf\r\n\r\n\r\n-0.7409699\r\n\r\n\r\n0.0702920\r\n\r\n\r\n-10.541307\r\n\r\n\r\n0.0000000\r\n\r\n\r\nsexu\r\n\r\n\r\n-0.4611341\r\n\r\n\r\n0.1188308\r\n\r\n\r\n-3.880592\r\n\r\n\r\n0.0001042\r\n\r\n\r\nOffsets\r\nWhen the data are grouped, it is important to consider the level of exposure in a given group. This can be achieved with an offset term (Piet de Jong, Gillian Z. Heller (2008), p66-67). This is important in an insurance context where we are often interested in modelling rates of claim.\r\n\r\n\r\nShow code\r\n\r\ndf_grp_filtered <- df_grp %>% filter(inc_count_tot>1, exposure>10)\r\nmodel_6 <- glm(inc_amount_tot~age+sex+offset(sum_assured),data=df_grp_filtered) \r\ntidy(model_6)\r\n\r\n\r\n# A tibble: 4 x 5\r\n  term         estimate std.error statistic  p.value\r\n  <chr>           <dbl>     <dbl>     <dbl>    <dbl>\r\n1 (Intercept) -3454137.   526285.     -6.56 2.39e-10\r\n2 age            50880.    10976.      4.64 5.37e- 6\r\n3 sexf          808044.   134595.      6.00 5.70e- 9\r\n4 sexu         1087955.   542771.      2.00 4.59e- 2\r\n\r\nShow code\r\n\r\nmodel_7 <- glm(inc_amount_tot~age+sex,data=df_grp_filtered) \r\ntidy(model_7)\r\n\r\n\r\n# A tibble: 4 x 5\r\n  term        estimate std.error statistic p.value\r\n  <chr>          <dbl>     <dbl>     <dbl>   <dbl>\r\n1 (Intercept)    8153.     5657.     1.44   0.151 \r\n2 age             204.      118.     1.73   0.0842\r\n3 sexf          -3454.     1447.    -2.39   0.0176\r\n4 sexu          -3801.     5834.    -0.652  0.515 \r\n\r\nActuals or AvE?\r\nThe models we have fitted above are based upon actual claim incidences. We can consider the difference between some expected claim rate (e.g. from a standard table) and the actuals.\r\nUsing the grouped data again, we model A/E with a normal response distribution. The model shows that none of the coeffients are significant indicating no material difference to expected, which is consistent with the data as the observations were derived from the expected probabilities initially.\r\n\r\n\r\nShow code\r\n\r\n# grouped model\r\nmodel_8 <- glm(inc_count_sick/inc_count_sick_exp~age+sex,data=df_grp) \r\n# use tidy to visualise the modelled result\r\nsummary(model_8)\r\n\r\n\r\n\r\nCall:\r\nglm(formula = inc_count_sick/inc_count_sick_exp ~ age + sex, \r\n    data = df_grp)\r\n\r\nDeviance Residuals: \r\n    Min       1Q   Median       3Q      Max  \r\n  -1.19    -0.97    -0.93    -0.87  2571.52  \r\n\r\nCoefficients:\r\n             Estimate Std. Error t value Pr(>|t|)\r\n(Intercept)  0.725747   0.456016   1.591    0.112\r\nage          0.004747   0.009713   0.489    0.625\r\nsexf        -0.091246   0.154067  -0.592    0.554\r\nsexu         0.174939   0.208211   0.840    0.401\r\n\r\n(Dispersion parameter for gaussian family taken to be 514.2801)\r\n\r\n    Null deviance: 56358742  on 109589  degrees of freedom\r\nResidual deviance: 56357894  on 109586  degrees of freedom\r\nAIC: 995154\r\n\r\nNumber of Fisher Scoring iterations: 2\r\n\r\nStepwise regression\r\nThis method builds a regression model from a set of candidate predictor variables by entering predictors based on p values, in a stepwise manner until there are no variables left to enter any more. Model may over/ understate the importance of predictors and the direction of stepping (forward or backward) can impact the outcome - so some degree of interpretation is necessary.\r\n\r\n\r\nShow code\r\n\r\n# specify a null model with no predictors\r\nnull_model_sick <- glm(inc_count_sick ~ 1, data = df_train, family = \"binomial\")\r\n\r\n# specify the full model using all of the potential predictors\r\nfull_model_sick <- glm(inc_count_sick ~ cal_year + policy_year + sex + smoker + benefit_period + waiting_period + occupation + poly(age,3) + sum_assured + policy_year*age + policy_year*sum_assured, data = df_train, family = \"binomial\")\r\n\r\n# alternatively, glm(y~ . -x1) fits model using all variables excluding x1\r\n\r\n# use a forward stepwise algorithm to build a parsimonious model\r\nstep_model_sick <- step(null_model_sick, scope = list(lower = null_model_sick, upper = full_model_sick), direction = \"forward\")\r\n\r\n\r\nStart:  AIC=20948.4\r\ninc_count_sick ~ 1\r\n\r\n                 Df Deviance   AIC\r\n+ age             1    20499 20503\r\n+ poly(age, 3)    3    20497 20505\r\n+ waiting_period  3    20581 20589\r\n+ policy_year     1    20833 20837\r\n+ sum_assured     1    20842 20846\r\n+ sex             2    20844 20850\r\n+ smoker          2    20925 20931\r\n+ cal_year        1    20932 20936\r\n<none>                 20946 20948\r\n+ benefit_period  2    20945 20951\r\n+ occupation      4    20941 20951\r\n\r\nStep:  AIC=20503.31\r\ninc_count_sick ~ age\r\n\r\n                 Df Deviance   AIC\r\n+ waiting_period  3    20135 20145\r\n+ sex             2    20398 20406\r\n+ smoker          2    20478 20486\r\n+ sum_assured     1    20482 20488\r\n<none>                 20499 20503\r\n+ policy_year     1    20498 20504\r\n+ cal_year        1    20499 20505\r\n+ poly(age, 3)    2    20497 20505\r\n+ occupation      4    20494 20506\r\n+ benefit_period  2    20498 20506\r\n\r\nStep:  AIC=20145.08\r\ninc_count_sick ~ age + waiting_period\r\n\r\n                 Df Deviance   AIC\r\n+ sex             2    20033 20047\r\n+ smoker          2    20114 20128\r\n+ sum_assured     1    20118 20130\r\n<none>                 20135 20145\r\n+ policy_year     1    20134 20146\r\n+ poly(age, 3)    2    20133 20147\r\n+ cal_year        1    20135 20147\r\n+ occupation      4    20129 20147\r\n+ benefit_period  2    20134 20148\r\n\r\nStep:  AIC=20047.43\r\ninc_count_sick ~ age + waiting_period + sex\r\n\r\n                 Df Deviance   AIC\r\n+ smoker          2    20012 20030\r\n+ sum_assured     1    20017 20033\r\n<none>                 20033 20047\r\n+ policy_year     1    20032 20048\r\n+ poly(age, 3)    2    20031 20049\r\n+ cal_year        1    20033 20049\r\n+ occupation      4    20028 20050\r\n+ benefit_period  2    20032 20050\r\n\r\nStep:  AIC=20030.01\r\ninc_count_sick ~ age + waiting_period + sex + smoker\r\n\r\n                 Df Deviance   AIC\r\n+ sum_assured     1    19995 20015\r\n<none>                 20012 20030\r\n+ policy_year     1    20011 20031\r\n+ poly(age, 3)    2    20009 20031\r\n+ cal_year        1    20012 20032\r\n+ occupation      4    20006 20032\r\n+ benefit_period  2    20011 20033\r\n\r\nStep:  AIC=20015.39\r\ninc_count_sick ~ age + waiting_period + sex + smoker + sum_assured\r\n\r\n                 Df Deviance   AIC\r\n<none>                 19995 20015\r\n+ policy_year     1    19994 20016\r\n+ poly(age, 3)    2    19993 20017\r\n+ cal_year        1    19995 20017\r\n+ benefit_period  2    19994 20018\r\n+ occupation      4    19995 20023\r\n\r\nShow code\r\n\r\nsummary(full_model_sick)\r\n\r\n\r\n\r\nCall:\r\nglm(formula = inc_count_sick ~ cal_year + policy_year + sex + \r\n    smoker + benefit_period + waiting_period + occupation + poly(age, \r\n    3) + sum_assured + policy_year * age + policy_year * sum_assured, \r\n    family = \"binomial\", data = df_train)\r\n\r\nDeviance Residuals: \r\n    Min       1Q   Median       3Q      Max  \r\n-0.2568  -0.0983  -0.0740  -0.0512   4.0416  \r\n\r\nCoefficients: (1 not defined because of singularities)\r\n                          Estimate Std. Error z value Pr(>|z|)    \r\n(Intercept)             -4.710e+01  6.626e+01  -0.711  0.47714    \r\ncal_year                 2.058e-02  3.283e-02   0.627  0.53070    \r\npolicy_year             -1.077e-01  1.018e-01  -1.058  0.28991    \r\nsexf                    -7.244e-01  8.052e-02  -8.997  < 2e-16 ***\r\nsexu                    -3.912e-01  1.333e-01  -2.934  0.00334 ** \r\nsmokers                  3.464e-01  7.424e-02   4.666 3.07e-06 ***\r\nsmokeru                 -1.076e-01  1.239e-01  -0.869  0.38493    \r\nbenefit_period2yr       -9.245e-02  8.108e-02  -1.140  0.25414    \r\nbenefit_period5yr       -3.854e-02  7.952e-02  -0.485  0.62796    \r\nwaiting_period14d       -2.306e-01  1.071e-01  -2.154  0.03121 *  \r\nwaiting_period720d      -1.837e+00  1.844e-01  -9.964  < 2e-16 ***\r\nwaiting_period90d       -1.595e+00  1.552e-01 -10.281  < 2e-16 ***\r\noccupationsed           -2.305e-02  9.765e-02  -0.236  0.81336    \r\noccupationtechn         -5.071e-02  1.022e-01  -0.496  0.61970    \r\noccupationwhite         -6.020e-02  1.013e-01  -0.594  0.55238    \r\noccupationblue          -3.262e-02  1.189e-01  -0.274  0.78383    \r\npoly(age, 3)1            3.006e+02  4.680e+01   6.423 1.34e-10 ***\r\npoly(age, 3)2           -4.075e+01  2.723e+01  -1.497  0.13445    \r\npoly(age, 3)3           -7.897e+00  2.002e+01  -0.394  0.69328    \r\nsum_assured              4.281e-05  2.744e-05   1.560  0.11867    \r\nage                             NA         NA      NA       NA    \r\npolicy_year:age          2.424e-03  2.110e-03   1.149  0.25072    \r\npolicy_year:sum_assured -4.251e-08  3.969e-06  -0.011  0.99145    \r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\n(Dispersion parameter for binomial family taken to be 1)\r\n\r\n    Null deviance: 20946  on 449999  degrees of freedom\r\nResidual deviance: 19987  on 449978  degrees of freedom\r\nAIC: 20031\r\n\r\nNumber of Fisher Scoring iterations: 9\r\n\r\nShow code\r\n\r\nsummary(step_model_sick)\r\n\r\n\r\n\r\nCall:\r\nglm(formula = inc_count_sick ~ age + waiting_period + sex + smoker + \r\n    sum_assured, family = \"binomial\", data = df_train)\r\n\r\nDeviance Residuals: \r\n    Min       1Q   Median       3Q      Max  \r\n-0.2830  -0.0973  -0.0745  -0.0519   4.0246  \r\n\r\nCoefficients:\r\n                     Estimate Std. Error z value Pr(>|z|)    \r\n(Intercept)        -9.640e+00  2.432e-01 -39.635  < 2e-16 ***\r\nage                 8.953e-02  4.755e-03  18.829  < 2e-16 ***\r\nwaiting_period14d  -2.295e-01  1.071e-01  -2.144  0.03202 *  \r\nwaiting_period720d -1.835e+00  1.844e-01  -9.951  < 2e-16 ***\r\nwaiting_period90d  -1.594e+00  1.552e-01 -10.273  < 2e-16 ***\r\nsexf               -7.238e-01  8.052e-02  -8.989  < 2e-16 ***\r\nsexu               -3.916e-01  1.333e-01  -2.937  0.00331 ** \r\nsmokers             3.466e-01  7.424e-02   4.669 3.03e-06 ***\r\nsmokeru            -1.067e-01  1.239e-01  -0.862  0.38885    \r\nsum_assured         4.333e-05  1.064e-05   4.072 4.66e-05 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\n(Dispersion parameter for binomial family taken to be 1)\r\n\r\n    Null deviance: 20946  on 449999  degrees of freedom\r\nResidual deviance: 19995  on 449990  degrees of freedom\r\nAIC: 20015\r\n\r\nNumber of Fisher Scoring iterations: 9\r\n\r\nThe form of the final step model is glm(formula = inc_count_sick ~ poly(age, 3) + waiting_period + sex + smoker + sum_assured, family = “binomial,” data = df_train) i.e. dropping some of the explanatory variables from the full model.\r\nConfidence intervals\r\nWe can compute the confidence intervals for one or more parameters in a fitted model.\r\n\r\n\r\nShow code\r\n\r\nconfint(step_model_sick) # add second argument specifying which parameters we need a confint for\r\n\r\n\r\n                           2.5 %        97.5 %\r\n(Intercept)        -1.011939e+01 -9.1658786725\r\nage                 8.021044e-02  0.0988490230\r\nwaiting_period14d  -4.333078e-01 -0.0131725684\r\nwaiting_period720d -2.205152e+00 -1.4802247926\r\nwaiting_period90d  -1.900406e+00 -1.2909882621\r\nsexf               -8.848852e-01 -0.5690645113\r\nsexu               -6.636738e-01 -0.1401306177\r\nsmokers             1.985219e-01  0.4896777597\r\nsmokeru            -3.586694e-01  0.1276681057\r\nsum_assured         2.248727e-05  0.0000642007\r\n\r\nPredictions\r\npredict() is a generic function that can be used to predict results from various model forms. The function takes the form below. For logistic regression, setting prediction type to response produces a probability rather than log odds (which are difficult to interpret).\r\n\r\n\r\nShow code\r\n\r\n# predictions\r\npred_inc_count_sick <- as.data.frame(\r\n  predict(step_model_sick, data= df_train, # model and data\r\n  type=\"response\", # or terms for coefficients\r\n  se.fit = TRUE, # default is false\r\n  interval = \"confidence\", #default \"none\", also \"prediction\"\r\n  level = 0.95\r\n  # ...\r\n  )\r\n)\r\n\r\n# add back to data\r\nifrm(\"df_train_pred\")\r\npred_inc_count_sick <- rename(pred_inc_count_sick,pred_rate_inc_count_sick=fit,se_rate_inc_count_sick=se.fit)\r\ndf_train_pred <- cbind(df_train,pred_inc_count_sick)\r\n\r\n\r\n\r\nWe can plot the results by age and sex against the crude rates from earlier:\r\n\r\n\r\nShow code\r\n\r\n# from earlier, summarise data by age and sex\r\ndf_train_pred %>% filter(sex != \"u\", between(age, 30,60)) %>% group_by(age,sex) %>% \r\nsummarise(total_sick=sum(inc_count_sick),total_acc=sum(inc_count_acc), pred_total_sick=sum(pred_rate_inc_count_sick),exposure=n(),.groups = 'drop') %>% \r\nmutate(sick_rate = total_sick/exposure,pred_sick_rate = pred_total_sick/exposure, acc_rate = total_acc/exposure) %>%\r\n# used ggplot to graph the results\r\nggplot(aes(x=age,y=sick_rate,color=sex)) +\r\n# ylim(0,1) +\r\ngeom_point() +\r\n# add a modeled line\r\ngeom_line(aes(x=age,y=pred_sick_rate)) +\r\ntheme_pander() + scale_color_gdocs()\r\n\r\n\r\n\r\n\r\nOut of sample predictions\r\nEarlier we split the data into “training” data used to create the model and “test” data which we intended to use for performance validation. Adding predictions to test data below.\r\n\r\n\r\nShow code\r\n\r\n# predictions\r\npred_inc_count_sick <- as.data.frame(\r\n  predict(step_model_sick, data= df_test, # test data\r\n  type=\"response\",\r\n  se.fit = TRUE, \r\n  interval = \"confidence\",\r\n  level = 0.95\r\n  # ...\r\n  )\r\n)\r\n\r\n# add back to data\r\nifrm(\"df_test_pred\")\r\npred_inc_count_sick <- rename(pred_inc_count_sick,pred_rate_inc_count_sick=fit,se_rate_inc_count_sick=se.fit)\r\ndf_test_pred <- cbind(df_test,pred_inc_count_sick)\r\n\r\n# summary stats for prediction\r\nhist(df_test_pred$pred_rate_inc_count_sick,main = \"Histogram of predicted sickness rate\",xlab = \"Probability of claim\")\r\n\r\n\r\n\r\nShow code\r\n\r\nsummary(df_test_pred$pred_rate_inc_count_sick)\r\n\r\n\r\n     Min.   1st Qu.    Median      Mean   3rd Qu.      Max. \r\n6.024e-05 1.363e-03 2.787e-03 3.498e-03 4.748e-03 3.925e-02 \r\n\r\nTo translate the predicted probabilities into a vector of claim/no-claim for each policy we could define a claim as occurring if modelled/ predicted probability of claim is greater than some threshold value. More on this later under evaluation techniques.\r\n\r\n\r\nShow code\r\n\r\n# add binary prediction based upon a threshold probability\r\ndf_test_pred$pred_inc_count_sick <- ifelse(df_test_pred$pred_rate_inc_count_sick>0.003,1,0) # example threshold is ~3rd quartile probability of claim. for balanced data this should be closer to 0.5.\r\n\r\n\r\n\r\nBayesian regression\r\nIn progress.\r\nOther classification models\r\nIn progress.\r\nEvaluation\r\nTechniques\r\nAIC\r\nFor least squares regression the \\(R^{2}\\) statistic (coefficient of determination) measures the proportion of variance in the dependent variable that can be explained by the independent variables. Adjusted \\(R^{2}\\), adjusts for the number of predictors in the model. The adjusted \\(R^{2}\\) increases when the new predictor improves the model more than would be expected by chance. The glm function uses a maximum likelihood estimator which does not minimize the squared error.\r\nAIC stands for Akaike Information Criteria. \\(AIC = -2l+2p\\) where \\(l\\) is the log likelihood and \\(p\\) are the number of parameters. It is analogous to adjusted \\(R^{2}\\) and is the measure of fit which penalizes model for the number of independent variables. We prefer a model with a lower AIC value.\r\n\r\n\r\nShow code\r\n\r\n# MASS package interferes with pairs() and cor(), reload here for stats tests later\r\nlibrary(\"MASS\")\r\nlibrary(\"arm\")\r\nlibrary(\"msme\")\r\n\r\n\r\n\r\nThe results below show the AIC for model 3 is lower than model 1 and that the final step model has the lowest AIC of those evaluated (preferred).\r\n\r\n\r\nShow code\r\n\r\nAIC(model_1)\r\n\r\n\r\n[1] 20503.31\r\n\r\nShow code\r\n\r\nAIC(model_3)\r\n\r\n\r\n[1] 20048.19\r\n\r\nShow code\r\n\r\nstepAIC(step_model_sick)\r\n\r\n\r\nStart:  AIC=20015.39\r\ninc_count_sick ~ age + waiting_period + sex + smoker + sum_assured\r\n\r\n                 Df Deviance   AIC\r\n<none>                 19995 20015\r\n- sum_assured     1    20012 20030\r\n- smoker          2    20017 20033\r\n- sex             2    20097 20113\r\n- age             1    20352 20370\r\n- waiting_period  3    20360 20374\r\n\r\nCall:  glm(formula = inc_count_sick ~ age + waiting_period + sex + smoker + \r\n    sum_assured, family = \"binomial\", data = df_train)\r\n\r\nCoefficients:\r\n       (Intercept)                 age   waiting_period14d  \r\n        -9.640e+00           8.953e-02          -2.295e-01  \r\nwaiting_period720d   waiting_period90d                sexf  \r\n        -1.835e+00          -1.594e+00          -7.238e-01  \r\n              sexu             smokers             smokeru  \r\n        -3.916e-01           3.466e-01          -1.067e-01  \r\n       sum_assured  \r\n         4.333e-05  \r\n\r\nDegrees of Freedom: 449999 Total (i.e. Null);  449990 Residual\r\nNull Deviance:      20950 \r\nResidual Deviance: 20000    AIC: 20020\r\n\r\nAnova\r\nAn anova comparison below between model_3 and the step model using a Chi-squared test shows a small p value for the stepped model - indicating that the model is an improvement. F-test can be used on continuous response models.\r\n\r\n\r\nShow code\r\n\r\nanova(model_3,step_model_sick, test=\"Chisq\")\r\n\r\n\r\nAnalysis of Deviance Table\r\n\r\nModel 1: inc_count_sick ~ age + policy_year + sex + waiting_period\r\nModel 2: inc_count_sick ~ age + waiting_period + sex + smoker + sum_assured\r\n  Resid. Df Resid. Dev Df Deviance  Pr(>Chi)    \r\n1    449992      20032                          \r\n2    449990      19995  2   36.795 1.023e-08 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nLikelihood ratio test\r\nThe likelihood ratio test compares two models based upon log likelihoods; more at Piet de Jong, Gillian Z. Heller (2008), p74.\r\nThe test below concludes that the step model is more accurate than the less complex model.\r\n\r\n\r\nShow code\r\n\r\nlrtest(model_3,step_model_sick)\r\n\r\n\r\nLikelihood ratio test\r\n\r\nModel 1: inc_count_sick ~ age + policy_year + sex + waiting_period\r\nModel 2: inc_count_sick ~ age + waiting_period + sex + smoker + sum_assured\r\n  #Df   LogLik Df  Chisq Pr(>Chisq)    \r\n1   8 -10016.1                         \r\n2  10  -9997.7  2 36.795  1.023e-08 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nOther tests\r\nOther validations tests could be considered like the Wald test, Score test; see Piet de Jong, Gillian Z. Heller (2008), p74-77.\r\nResidual checks\r\nResiduals/ errors are the observed less fitted values. Traditional residual plots (shown below) are usually a good starting point (we expect to see no trend in the plot of residuals vs fitted values), but are not as informative for logistic regression or for data with a low probability outcome.\r\nStandard model plots\r\nPlots, linear regression example:\r\n\r\n\r\nShow code\r\n\r\npar(mfrow = c(2, 2))\r\nplot(amount_model_3)\r\n\r\n\r\n\r\n\r\nLogistic regression example:\r\n\r\n\r\nShow code\r\n\r\npar(mfrow = c(2, 2))\r\nplot(step_model_sick)\r\n\r\n\r\n\r\n\r\nAlternatives\r\nFor logistic regression we can try other tools to test the residuals against the model assumptions. GLMs assume that the residuals/ errors are normally distributed. Plotting the density of the residuals gives us:\r\n\r\n\r\nShow code\r\n\r\nhist(rstandard(step_model_sick),breaks= c(seq(-1,1,by=0.001),seq(2,5, by=1)),freq=FALSE,main = \"Histogram of residuals\",xlab = \"Residuals\")\r\ncurve(dnorm, add = TRUE)\r\n\r\n\r\n\r\n\r\nFocusing the x axis range:\r\n\r\n\r\nShow code\r\n\r\nhist(rstandard(step_model_sick),breaks= c(seq(-1,1,by=0.001),seq(2,5, by=1)), xlim = c(-0.5,0.5),freq=FALSE,main = \"Histogram of residuals\",xlab = \"Residuals\")\r\n\r\n\r\n\r\n\r\nA binned residual plot divides data into bins based upon their fitted values, showing the average residuals vs fitted value for each bin (Jeff Webb 2017):\r\n\r\n\r\nShow code\r\n\r\n# from the arm package\r\nbinnedplot(fitted(step_model_sick), \r\n           residuals(step_model_sick, type = \"response\"), \r\n           nclass = 50, \r\n           xlab = \"Expected Values\", \r\n           ylab = \"Average residual\", \r\n           main = \"Binned residual plot\", \r\n           cex.pts = 0.8, \r\n           col.pts = 1, \r\n           col.int = \"gray\")\r\n\r\n\r\n\r\n\r\nGrey lines are 2 se bands (~95%). Apart from a few outliers, most of the residuals are within those bands.\r\nP-P plots\r\nThe P-P (probability–probability) plot is a visualization that plots CDFs of the two distributions (empirical and theoretical) against each other, an unrelated dummy example below. It can be used to assess the residuals for normality.\r\n\r\n\r\nShow code\r\n\r\nx <- rnorm(100)\r\nprobDist <- pnorm(x)\r\n#create PP plot\r\nplot(ppoints(length(x)), sort(probDist), main = \"PP Plot\", xlab = \"Observed Probability\", ylab = \"Expected Probability\")\r\n#add diagonal line\r\nabline(0,1)\r\n\r\n\r\n\r\n\r\nOther tests\r\nPearson dispersion test: This function calculates Pearson Chi2 statistic and the Pearson-based dipersion statistic. Values of the dispersion greater than 1 indicate model overdispersion. Values under 1 indicate under-dispersion.\r\n\r\n\r\nShow code\r\n\r\nP__disp(step_model_sick)\r\n\r\n\r\npearson.chi2   dispersion \r\n457855.78948      1.01748 \r\n\r\nConfusion matrix\r\nWith a binary prediction from the model loaded within the dataframe (defined earlier), we can compare this to the actual outcomes to determine the validity of the model. In this comparison, the true positive rate is called the Sensitivity. The inverse of the false-positive rate is called the Specificity.\r\nSensitivity = TruePositive / (TruePositive + FalseNegative)\r\nSpecificity = TrueNegative / (FalsePositive + TrueNegative)\r\nWhere:\r\nSensitivity = True Positive Rate\r\nSpecificity = 1 – False Positive Rate\r\nA perfect classification model could have Sensitivity and Specificity close to 1. However, we noted earlier, in insurance applications we are not often interested in in an accurate prediction as to whether a given policy gives rise to a claim. Rather we are interested in understanding the claim rates and how they are explained by the response variables/ risk factors (Piet de Jong, Gillian Z. Heller 2008, p108–109).\r\nA confusion matrix is a tabular representation of Observed vs Predicted values. It helps to quantify the efficiency (or accuracy) of the model.\r\n\r\n\r\nShow code\r\n\r\n# MASS package interferes with pairs() and cor() and others\r\nrequire(arm) # package\r\ndetach(package:arm)\r\ndetach(package:msme)\r\ndetach(package:MASS)\r\n\r\n\r\n\r\n\r\n\r\nShow code\r\n\r\nconfusion_matrix <- df_test_pred %>% select(inc_count_sick,pred_inc_count_sick) %>% table()\r\nconfusion_matrix\r\n\r\n\r\n              pred_inc_count_sick\r\ninc_count_sick      0      1\r\n             0 239855 208606\r\n             1    808    731\r\n\r\n\r\n\r\nShow code\r\n\r\ncat(\"Accuracy Rate =\",(confusion_matrix[1,1]+confusion_matrix[2,2])/sum(confusion_matrix[]),\r\n\"; Missclasification Rate =\",(confusion_matrix[1,2]+confusion_matrix[2,1])/sum(confusion_matrix[]),\r\n\"; True Positive Rate/Sensitivity  =\",confusion_matrix[2,2]/sum(confusion_matrix[2,]),\r\n\"; False Positive Rate =\",confusion_matrix[1,2]/sum(confusion_matrix[1,]),\r\n\"; Specificity =\",1-confusion_matrix[1,2]/sum(confusion_matrix[1,]))\r\n\r\n\r\nAccuracy Rate = 0.5346356 ; Missclasification Rate = 0.4653644 ; True Positive Rate/Sensitivity  = 0.4749838 ; False Positive Rate = 0.4651597 ; Specificity = 0.5348403\r\n\r\nROC/ AUC\r\nThe ROC (Receiver Operating Characteristic) curve is a graph with:\r\nThe x-axis showing the False Positive Rate\r\nThe y-axis showing the True Positive Rate\r\nROC curves start at 0 on the x and y axis and rise to 1. The faster the curve reaches a True Positive Rate of 1, the better the curve generally. A model on the diagonal is only showing a 50/50 chance of correctly guessing the probability of claim. Area under an ROC curve (AUC) is a measure of the usefulness of a model in general, where a greater area means more useful. AUC is a tool for comparing models (generally, the closer the AUC is to 1, the better, but there are some cases where AUC can be misleading; AUC = 0.5 is a model on the diagonal).\r\nIn our binary model, the AUC is not much better than 0.5, indicating a very weak predictive ability. It isn’t hugely surprising that our model is not very effective at predicting individual claims. As noted earlier, for insurance applications, we are usually more concerned with predicting the claim rate and how it varies by predictors like age and sex (Piet de Jong, Gillian Z. Heller 2008, p108–109).\r\n\r\n\r\nShow code\r\n\r\npar(mfrow = c(1,1))\r\nroc = roc(df_test_pred$inc_count_sick, df_test_pred$pred_rate_inc_count_sick, plot = TRUE, print.auc = TRUE)\r\n\r\n\r\n\r\nShow code\r\n\r\nas.numeric(roc$auc)\r\n\r\n\r\n[1] 0.5074699\r\n\r\nShow code\r\n\r\ncoords(roc, \"best\", ret = \"threshold\")\r\n\r\n\r\n    threshold\r\n1 0.001932091\r\n\r\nReferences\r\n\r\n\r\nIan Welch. 2020. “Joint Study Reveals Large Rise in Life Insurance Claims Costs.” https://home.kpmg/au/en/home/media/press-releases/2020/06/joint-study-reveals-large-rise-life-insurance-claims-costs-22-june-2020.html.\r\n\r\n\r\nJames Louw. 2012. “Disability Income ProductInnovation in Australia.” Australia: Gen Re. http://www.actuaries.org/HongKong2012/Presentations/WBR5_Louw.pdf.\r\n\r\n\r\nJeff Webb. 2017. “Statistics and Predictive Analytics.” https://bookdown.org/jefftemplewebb/IS-6489/logistic-regression.html#assessing-logistic-model-fit.\r\n\r\n\r\nPiet de Jong, Gillian Z. Heller. 2008. Generalised Linear Models for Insurance Data. Cambridge University Press. https://ggplot2.tidyverse.org/reference/ggtheme.html.\r\n\r\n\r\npkgdown. n.d.a. “Create a New Ggplot.” https://ggplot2.tidyverse.org/reference/ggplot.html.\r\n\r\n\r\n———. n.d.b. “Ggplot Themes.” https://ggplot2.tidyverse.org/reference/ggtheme.html.\r\n\r\n\r\nSOA. 2019. “Analysis of Claim Incidence Experience from 2006 to 2014.” USA: SOA. https://www.soa.org/resources/experience-studies/2019/claim-incidence-report/.\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/20211014 regression/preview.png",
    "last_modified": "2021-11-19T16:09:19+11:00",
    "input_file": "regression.knit.md",
    "preview_width": 766,
    "preview_height": 480
  }
]
